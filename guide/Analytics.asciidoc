=== Introduction

Spring XD provides support for the real-time evaluation of various machine learning scoring algorithms as well simple real-time data analytics using various types of counters and gauges.  Spring XD provides this functionalty as an extensible class library with a few out of the box implemenations.  

For Machine Learning algorithms, we integrate with the https://github.com/jpmml/jpmml-evaluator[JPMML-Evaluator] library that provides support for a wide range of https://github.com/jpmml/jpmml-evaluator#features[model types] and is interopable with models exported from http://www.r-project.org/[R], http://rattle.togaware.com/[Rattle], http://www.knime.org/[KNIME], and http://rapid-i.com/content/view/181/190/[RapidMiner].  For counter and gauge analytics, in-memory and http://redis.io/[Redis] implementations are provided.

Incorporating the evaluation of machine learning algorithms into stream processing is as easy as using any other processing module.  Here is a simple example

----
http --outputType=application/x-xd-tuple | analytic-pmml --location=/models/iris-flower-naive-bayes.pmml.xml
     	 --inputFieldMapping='sepalLength:Sepal.Length,sepalWidth:Sepal.Width,petalLength:Petal.Length,petalWidth:Petal.Width' 
	 --outputFieldMapping='Predicted_Species:predictedSpecies' | log"
----

The `http` source converts posted data to a Tuple.  The `analytic-pmml` processor loads the model from the specifed file and creates two mappings so that fields from the Tuple can be mapped into the input and output model names.  The `log` sink writes the payload of the event message to the log file of the XD container.

Posting the following JSON data to the http source
[source,json]
----
{ 
  "sepalLength": "6.4", 
  "sepalWidth":  "3.2", 
  "petalLength": "4.5", 
  "petalWidth":  "1.5" 
}
----

will produce output in the log file as shown below.
[source,json]
----
{
   "id":"1722ec00-baad-11e3-b988-005056c00008",
   "timestamp":1396473833152,
   "sepalLength":"6.4",
   "sepalWidth":"3.2",
   "petalLength":"4.5",
   "petalWidth":"1.5",
   "predictedSpecies":"versicolor"
}
----

The next section on analytical models goes into more detail on the general infrastructure 

=== Analytical Models

We provide some core abstractions for implementing analytical models in stream processing applications.
The main interface for integrating analytical models is *Analytic*. Some analytical
models need to adjust the domain input and the model output in some way, therefore we provide a special base class *MappedAnalytic*
which has core abstractions for implementing that mapping via *InputMapper* and *OutputMapper*.

Since *Spring XD 1.0.0.M6* we support the integration of analytical models, also called statistical models or mining models, that are defined via http://en.wikipedia.org/wiki/Predictive_Model_Markup_Language[*PMML*].
*PMML* is the abreviation for *Predictive Model Markup Language* and is a standard XML representation that allows specifications of different mining models, their ensembles, and associated preprocessing. 

[NOTE]
=====================================================================
*PMML* is maintained by the *Data Mining Group* (*DMG*) and supported by several state-of-the-art statistics and data mining software tools such as InfoSphere Warehouse, R / Rattle, SAS Enterprise Miner, SPSSÂ®, and Weka. 
The current version of the *PMML* specification is http://www.dmg.org/v4-2/GeneralStructure.html[*4.2*] at the time of this writing.
Applications can produce and consume *PMML* models, thus allowing an analytical model created in one application to be implemented and used for scoring or prediction in another.
=====================================================================

*PMML* is just one of many other technologies that one can integrate to implement analytics with, more will follow in upcoming releases.

==== Modelling and Evaluation
Analytical models are usually defined by a statistician _aka_ data scientist or quant by using some statistical tool to analyze the data and build an appropriate model.
In order to implement those models in a business application they are usually transformed and exported in some way (_e.g._ in the form of a *PMML* definition).
This model is then loaded into the application which then evaluates it against a given input (event, tuple, example).

==== Modeling
Analytical models can be defined in various ways. For the sake of brevity we use *R* from the http://www.r-project.org[*r-project*] to demonstrate
how easy it is to export an analytical model to *PMML* and use it later in stream processing.

For our example we use the http://en.wikipedia.org/wiki/Iris_flower_data_set[*iris*] example dataset in *R* to generate a classifier for iris flower species by applying the http://en.wikipedia.org/wiki/Naive_Bayes_classifier[*Naive Bayes*] algorithm.

[source,r]
----
library(e1071) # Load library with the naive bayes algorithm support.

library(pmml) # Load library with PMML export support.

data(iris) # Load the IRIS example dataset

#Helper function to split the given dataset into a dataset used for training (trainset) and (testset) used for evaulation.
splitDataFrame <- function(dataframe, seed = NULL, n = trainSize) {

   if (!is.null(seed)){
      set.seed(seed)
   }

   index <- 1:nrow(dataframe)
   trainindex <- sample(index, n)
   trainset <- dataframe[trainindex, ]
   testset <- dataframe[-trainindex, ]

   list(trainset = trainset, testset = testset)
}

#We want to use 95% of the IRIS data as training data and 5% as test data for evaluation.
datasets <- splitDataFrame(iris, seed = 1337, n= round(0.95 * nrow(iris)))

#Create a naive Bayes classifier to predict iris flower species (iris[,5]) from [,1:4] = Sepal.Length Sepal.Width Petal.Length Petal.Width
model <- naiveBayes(datasets$trainset[,1:4], datasets$trainset[,5])

#The name of the model and it's externalId could be used to uniquely identify this version of the model.
modelName = "iris-flower-classifier"
externalId = 42

#Convert the given model into a PMML model definition
pmmlDefinition = pmml.naiveBayes(model,model.name=paste(modelName,externalId,sep = ";"), predictedField='Species')

#Print the PMML definition to stdout
cat(toString(pmmlDefinition))
----

The r script above should produce the following *PMML* document that contains the abstract definition of the naive bayes classifer that we derived
from the training dataset of the IRIS dataset.
[source, xml]
----
<PMML version="4.1" xmlns="http://www.dmg.org/PMML-4_1" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.dmg.org/PMML-4_1 http://www.dmg.org/v4-1/pmml-4-1.xsd">
<Header copyright="Copyright (c) 2014 tom" description="NaiveBayes Model">
 <Extension name="user" value="tom" extender="Rattle/PMML"/>
 <Application name="Rattle/PMML" version="1.4"/>
 <Timestamp>2014-04-02 13:22:15</Timestamp>
</Header>
<DataDictionary numberOfFields="6">
 <DataField name="Species" optype="categorical" dataType="string">
  <Value value="setosa"/>
  <Value value="versicolor"/>
  <Value value="virginica"/>
 </DataField>
 <DataField name="Sepal.Length" optype="continuous" dataType="double"/>
 <DataField name="Sepal.Width" optype="continuous" dataType="double"/>
 <DataField name="Petal.Length" optype="continuous" dataType="double"/>
 <DataField name="Petal.Width" optype="continuous" dataType="double"/>
 <DataField name="DiscretePlaceHolder" optype="categorical" dataType="string">
  <Value value="pseudoValue"/>
 </DataField>
</DataDictionary>
<NaiveBayesModel modelName="iris-flower-classifier;42" functionName="classification" threshold="0.001">
 <MiningSchema>
  <MiningField name="Species" usageType="predicted"/>
  <MiningField name="Sepal.Length" usageType="active"/>
  <MiningField name="Sepal.Width" usageType="active"/>
  <MiningField name="Petal.Length" usageType="active"/>
  <MiningField name="Petal.Width" usageType="active"/>
  <MiningField name="DiscretePlaceHolder" usageType="active" missingValueReplacement="pseudoValue"/>
 </MiningSchema>
 <Output>
  <OutputField name="Predicted_Species" feature="predictedValue"/>
  <OutputField name="Probability_setosa" optype="continuous" dataType="double" feature="probability" value="setosa"/>
  <OutputField name="Probability_versicolor" optype="continuous" dataType="double" feature="probability" value="versicolor"/>
  <OutputField name="Probability_virginica" optype="continuous" dataType="double" feature="probability" value="virginica"/>
 </Output>
 <BayesInputs>
  <Extension>
   <BayesInput fieldName="Sepal.Length">
    <TargetValueStats>
     <TargetValueStat value="setosa">
      <GaussianDistribution mean="5.006" variance="0.124248979591837"/>
     </TargetValueStat>
     <TargetValueStat value="versicolor">
      <GaussianDistribution mean="5.8953488372093" variance="0.283311184939092"/>
     </TargetValueStat>
     <TargetValueStat value="virginica">
      <GaussianDistribution mean="6.58163265306122" variance="0.410697278911565"/>
     </TargetValueStat>
    </TargetValueStats>
   </BayesInput>
  </Extension>
  <Extension>
   <BayesInput fieldName="Sepal.Width">
    <TargetValueStats>
     <TargetValueStat value="setosa">
      <GaussianDistribution mean="3.428" variance="0.143689795918367"/>
     </TargetValueStat>
     <TargetValueStat value="versicolor">
      <GaussianDistribution mean="2.76279069767442" variance="0.0966777408637874"/>
     </TargetValueStat>
     <TargetValueStat value="virginica">
      <GaussianDistribution mean="2.97142857142857" variance="0.105833333333333"/>
     </TargetValueStat>
    </TargetValueStats>
   </BayesInput>
  </Extension>
  <Extension>
   <BayesInput fieldName="Petal.Length">
    <TargetValueStats>
     <TargetValueStat value="setosa">
      <GaussianDistribution mean="1.462" variance="0.0301591836734694"/>
     </TargetValueStat>
     <TargetValueStat value="versicolor">
      <GaussianDistribution mean="4.21627906976744" variance="0.236633444075305"/>
     </TargetValueStat>
     <TargetValueStat value="virginica">
      <GaussianDistribution mean="5.55510204081633" variance="0.310442176870748"/>
     </TargetValueStat>
    </TargetValueStats>
   </BayesInput>
  </Extension>
  <Extension>
   <BayesInput fieldName="Petal.Width">
    <TargetValueStats>
     <TargetValueStat value="setosa">
      <GaussianDistribution mean="0.246" variance="0.0111061224489796"/>
     </TargetValueStat>
     <TargetValueStat value="versicolor">
      <GaussianDistribution mean="1.30697674418605" variance="0.042093023255814"/>
     </TargetValueStat>
     <TargetValueStat value="virginica">
      <GaussianDistribution mean="2.02448979591837" variance="0.0768877551020408"/>
     </TargetValueStat>
    </TargetValueStats>
   </BayesInput>
  </Extension>
  <BayesInput fieldName="DiscretePlaceHolder">
   <PairCounts value="pseudoValue">
    <TargetValueCounts>
     <TargetValueCount value="setosa" count="50"/>
     <TargetValueCount value="versicolor" count="43"/>
     <TargetValueCount value="virginica" count="49"/>
    </TargetValueCounts>
   </PairCounts>
  </BayesInput>
 </BayesInputs>
 <BayesOutput fieldName="Species">
  <TargetValueCounts>
   <TargetValueCount value="setosa" count="50"/>
   <TargetValueCount value="versicolor" count="43"/>
   <TargetValueCount value="virginica" count="49"/>
  </TargetValueCounts>
 </BayesOutput>
</NaiveBayesModel>
</PMML>
----

==== Evaluation

The above defined *PMML* model can be evaluated in a Spring XD stream definition by using the *analytic-pmml* module as a processor
in your stream definition. The actual evaluation of the *PMML* is performed via the *PmmlAnalytic* which uses the https://github.com/jpmml/jpmml-evaluator[*jpmml-evaluator*] library.

==== Model Selection

The PMML standard allows multiple models to be defined within a single PMML document.
The model to be used can be configured through the *modelName* option.

*NOTE* The PMML standard also supports other ways for selection models, _e.g._ based on a predicate. This is currently not supported.

In order to perform the evaluation in Spring XD you need to save the generated PMML document to some folder, typically the with the extension "pmml.xml".
For this example we save the PMML document under the name *iris-flower-classification-naive-bayes-1.pmml.xml*.

In the following example we set up a stream definition with an `http` source that produces iris-flower-records
that are piped to the `analytic-pmml` module which applies our iris flower classifier to predict the species of a given flower record.
The result of that is a new record extended by a new attribute *predictedSpecies* which simply sent to a `log` sink.

The definition of the stream, which we call *iris-flower-classification*, looks as follows:

----
xd:>stream create --name iris-flower-classification --definition "http --outputType=application/x-xd-tuple | analytic-pmml --location=/models/iris-flower-classification-naive-bayes-1.pmml.xml --inputFieldMapping='sepalLength:Sepal.Length,sepalWidth:Sepal.Width,petalLength:Petal.Length,petalWidth:Petal.Width' --outputFieldMapping='Predicted_Species:predictedSpecies' | log" --deploy
----

* The *location* parameter can be used to specify the exact location of the pmml document. The value must be a valid spring http://www.springindepth.com/2.5.x/0.10/ch05.html[*resource*] location
* The *inputFieldMapping* parameter defines a mapping of domain input fields to model input fields. It is just a list of fields or optional field:alias mappings to control which fields and how they are going to end up in the model-input. If no inputFieldMapping is defined then all domain input fields are used as model input. +
* The *outputFieldMapping* parameter defines a mapping of model output fields to domain output fields with semantics analog to the inputFieldMapping. +
* The optional *modelName* parameter of the analytic-pmml module can be used to refer to a particular named model within the PMML definition. If modelName is not defined the first model is selected by default. +

*NOTE* Some analytical models like for instance *association rules* require a different typ of mapping. You can implement your own custom mapping strategies by implementing a custom *InputMapper* and *OutputMapper*
and defining a new *PmmlAnalytic* or *TuplePmmlAnalytic* bean that uses your custom mappers.

After the stream has been successfuly deployed to *Spring XD* we can eventually start to throw some data at it by issuing the follwing http request via the *XD-Shell* (or `curl`, or any other tool):

*Note* that our example record contains no information about which species the example belongs to - this will be added by our classifier.

----
xd:>http post --target http://localhost:9000 --contentType application/json --data "{ \"sepalLength\": 6.4, \"sepalWidth\": 3.2, \"petalLength\":4.5, \"petalWidth\":1.5 }"
----

After posting the above json document to the stream we should see the following output in the console:
[source, json]
----
   {
     "id":"1722ec00-baad-11e3-b988-005056c00008"
   , "timestamp":1396473833152
   , "sepalLength":"6.4"
   , "sepalWidth":"3.2"
   , "petalLength":"4.5"
   , "petalWidth":"1.5"
   , "predictedSpecies":"versicolor"
   }
----

*NOTE* the generated field *predictedSpecies* which now identifies our input as belonging to the iris species *versicolor*.

We verify that the generated *PMML* classifier produces the same result as *R* by executing the issuing the following commands in *rproject*:
[source,r]
----
datasets$testset[,1:4][1,]
# This is the first example record that we sent via the http post.
   Sepal.Length Sepal.Width Petal.Length Petal.Width
52          6.4         3.2          4.5         1.5

#Predict the class for the example record by using our naiveBayes model.
> predict(model, datasets$testset[,1:4][1,])
[1] versicolor
----



=== Counters and Gauges
Counter and Gauges are analytical data strucutres collectively referred to as metrics.  Metrics can be used directly in place of a sink just as if you were creating any other link:Streams#streams[stream], but you can also analyze data from an existing stream using a link:Taps#taps[tap]. We'll look at some examples of using metrics with taps in the following sections. As a prerequisite start the XD Container as instructed in the link:Getting-Started#getting-started[Getting Started] page. 

The 1.0 release provides the following types of metrics

* <<counter,Counter>>
* <<field-value-counter,Field Value Counter>>
* <<aggregate-counter, Aggregate Counter>>
* <<gauge,Gauge>>
* <<rich-gauge,Rich Gauge>>


Spring XD supports these metrics and analytical data structures as a general purpose class library that works with several backend storage technologies.  The 1.0 release provides in memory and Redis implementations.


[[counter]]
==== Counter

A counter is a Metric that associates a unique name with a long value. It is primarily used for counting events triggered by incoming messages on a target stream. You create a counter with a unique name and optionally an initial value then set its value in response to incoming messages. The most straightforward use for counter is simply to count messages coming into the target stream. That is, its value is incremented on every message. This is exactly what the _counter_ module provided by Spring XD does. 

Here's an example:

Start by creating a data ingestion stream. Something like:

   xd:> stream create --name springtweets --definition "twittersearch --consumerKey=<your_key> --consumerSecret=<your_secret> --query=spring | file --dir=/tweets/" --deploy

Next, create a tap on the _springtweets_ stream that sets a message counter named _tweetcount_

   xd:> stream create --name tweettap --definition "tap:stream:springtweets > counter --name=tweetcount" --deploy

The results are written to redis under the key counter.${name}. To retrieve the count:
  
   $ redis-cli
   redis 127.0.0.1:6379> get counters.tweetcount

[[field-value-counter]]
==== Field Value Counter

A field value counter is a Metric used for counting occurrences of unique values for a named field in a message payload. XD Supports the following payload types out of the box:

* POJO (Java bean)
* Tuple
* JSON String

For example suppose a message source produces a payload with a field named _user_ :

[source,java]
class Foo {
   String user;
   public Foo(String user) {
       this.user = user;
   }
}

If the stream source produces messages with the following objects:

[source, java]
   new Foo("fred")
   new Foo("sue")
   new Foo("dave")
   new Foo("sue")

The field value counter on the field _user_ will contain:

    fred:1, sue:2, dave:1 

Multi-value fields are also supported. For example, if a field contains a list, each value will be counted once:
    
     users:["dave","fred","sue"]
     users:["sue","jon"]

The field value counter on the field _users_ will contain:

    dave:1, fred:1, sue:2, jon:1


field_value_counter has the following options:

fieldName:: The name of the field for which values are counted *(required)*
name:: A key used to access the counter values. *(default: stream name)*

To try this out, create a stream to ingest twitter feeds containing the word _spring_ and output to a file:

   xd:> stream create --name springtweets --definition "twittersearch --consumerKey=<your_key> --consumerSecret=<your_secret> --query=spring | file" --deploy

Now create a tap for a field value counter:

   xd:> stream create --name fromUserCount --definition "tap:stream:springtweets > field-value-counter --fieldName=fromUser" --deploy

The _twittersearch_ source produces JSON strings which contain the user id of the tweeter in the _fromUser_ field. The _field_value_counter_ sink parses the tweet and updates a field value counter named _fromUserCount_ in Redis. To view the counts:

   $ redis-cli
   redis 127.0.0.1:6379>zrange fieldvaluecounters.fromUserCount 0 -1 withscores 

[[aggregate-counter]]
==== Aggregate Counter

The aggregate counter differs from a simple counter in that it not only keeps a total value for the count, but also retains the total count values for each minute, hour day and month of the period for which it is run. The data can then be queried by supplying a start and end date and the resolution at which the data should be returned. 

Creating an aggregate counter is very similar to a simple counter. For example, to obtain an aggregate count for our spring tweets stream:
   
    xd:> stream create --name springtweets --definition "twittersearch --query=spring | file" --deploy

you'd simply create a tap which pipes the input to `aggregate-counter`:

   xd:> stream create --name tweettap --definition "tap:stream:springtweets > aggregate-counter --name=tweetcount" --deploy

The Redis back-end stores the aggregate counts in buckets prefixed with `aggregatecounters.${name}`. The rest of the string contains the date information. So for our `tweetcount` counter you might see something like the following keys appearing in Redis:

    redis 127.0.0.1:6379> keys aggregatecounters.tweetcount*
    1) "aggregatecounters.tweetcount"
    2) "aggregatecounters.tweetcount.years"
    3) "aggregatecounters.tweetcount.2013"
    4) "aggregatecounters.tweetcount.201307"
    5) "aggregatecounters.tweetcount.20130719"
    6) "aggregatecounters.tweetcount.2013071914"

The general format is
    
    . One total value
    . One years hash with a field per year eg. { 2010: value, 2011: value }
    . One hash per year with a field per month { 01: value, ...}
    . One hash per month with a field per day
    . One hash per day with a field per hour
    . One hash per hour with a field per minute


[[gauge]]
==== Gauge

A gauge is a Metric, similar to a counter in that it holds a single long value associated with a unique name. In this case the value can represent any numeric value defined by the application. 

The _gauge_ sink provided with XD stores expects a numeric value as a payload, typically this would be a decimal formatted string, and stores its values in Redis. The gauge includes the following attributes:

 name:: The name for the gauge *(default: `<streamname>`)*

===== Note:

When using gauges and rich gauges with these examples you will need a redis instance running.  Also if you are using singlenode, start your single node with the --analytics redis parameter 
----
xd-singlenode --analyttics redis
----

Here is an example of creating a tap for a gauge:

===== Simple Tap Example

Create an ingest stream

    xd:> stream create --name test --definition "http --port=9090 | file" --deploy

Next create the tap:

    xd:> stream create --name simplegauge --definition "tap:stream:test > gauge" --deploy

Now Post a message to the ingest stream:

    xd:> http post --target http://localhost:9090 --data "10"

Check the gauge:

    $ redis-cli
    redis 127.0.0.1:6379> get gauges.simplegauge
    "10"

[[rich-gauge]]
==== Rich Gauge

A rich gauge is a Metric that holds a double value associated with a unique name. In addition to the value, the rich gauge keeps a running average, along with the minimum and maximum values and the sample count.

The _richgauge_ sink provided with XD expects a numeric value as a payload, typically this would be a decimal formatted string, and keeps its value in a store. The rich-gauge includes the following attributes:

 name:: The name for the gauge *(default: `<streamname>`)*
 alpha:: A smoothing factor between 0 and 1, that if set will compute an http://en.wikipedia.org/wiki/Exponential_smoothing[exponential moving average] *(default: `-1, simple average`)* 

When stored in Redis, the values are kept as a space delimited string, formatted as _value_ _alpha_ _mean_ _max_ _min_ _count_

Here are some examples of creating a tap for a rich gauge:

===== Simple Tap Example

Create an ingest stream

      xd:> stream create --name test --definition "http --port=9090 | file" --deploy

Next create the tap:

      xd:> stream create --name testgauge --definition "tap:stream:test > rich-gauge" --deploy

Now Post some messages to the ingest stream:

    xd:> http post --target http://localhost:9090 --data "10"
    xd:> http post --target http://localhost:9090 --data "13"
    xd:> http post --target http://localhost:9090 --data "16"

Check the gauge:

    $ redis-cli
    redis 127.0.0.1:6379> get richgauges.testgauge
    "16.0 -1 13.0 16.0 10.0 3"

===== Stock Price Example

In this example, we will track stock prices, which is a more practical example. The data is ingested as JSON strings like 

    {"symbol":"VMW","price":72.04}


Create an ingest stream

     xd:> stream create --name stocks --definition "http --port=9090 | file"

Next create the tap, using the transform module to extract the stock price from the payload: 

     xd:> stream create --name stockprice --definition "tap:stream:stocks > transform --expression=#jsonPath(payload,'$.price') | rich-gauge"

Now Post some messages to the ingest stream:

    xd:> http post --target http://localhost:9090 --data {"symbol":"VMW","price":72.04}
    xd:> http post --target http://localhost:9090 --data {"symbol":"VMW","price":72.06}
    xd:> http post --target http://localhost:9090 --data {"symbol":"VMW","price":72.08}

Note: JSON fields should be separated by a comma without any spaces. Alternatively, enclose the whole argument to `--data` with quotes and escape inner quotes with a backslash.

Check the gauge:

    $ redis-cli
    redis 127.0.0.1:6379> get richgauges.stockprice
    "72.08 -1 72.04 72.08 72.02 3"


===== Improved Stock Price Example

In this example, we will track stock prices for selected stocks. The data is ingested as JSON strings like 

    {"symbol":"VMW","price":72.04}
    {"symbol":"EMC","price":24.92}

The previous example would feed these prices to a single gauge. What we really want is to create a separate tap for each ticker symbol in which we are interested:

Create an ingest stream

     xd:> stream create --name stocks --definition "http --port=9090 | file"

Next create the tap, using the transform module to extract the stock price from the payload: 

     xd:> stream create --name vmwprice --definition "tap:stream:stocks > filter --expression=#jsonPath(payload,'$.symbol')==VMW | transform --expression=#jsonPath(payload,'$.price') | rich-gauge" --deploy
     xd:> stream create --name emcprice --definition "tap:stream:stocks > filter --expression=#jsonPath(payload,'$.symbol')==EMC | transform --expression=#jsonPath(payload,'$.price') | rich-gauge" --deploy

Now Post some messages to the ingest stream:

    xd:> http post --target http://localhost:9090 --data {"symbol":"VMW","price":72.04}
    xd:> http post --target http://localhost:9090 --data {"symbol":"VMW","price":72.06}
    xd:> http post --target http://localhost:9090 --data {"symbol":"VMW","price":72.08}

    xd:> http post --target http://localhost:9090 --data {"symbol":"EMC","price":24.92}
    xd:> http post --target http://localhost:9090 --data {"symbol":"EMC","price":24.90}
    xd:> http post --target http://localhost:9090 --data {"symbol":"EMC","price":24.96}

Check the gauge:

    $ redis-cli
    redis 127.0.0.1:6379> get richgauges.emcprice
    "24.96 -1 24.926666666666666 24.96 24.9 3"
    
    redis 127.0.0.1:6379> get richgauges.vmwprice
    "72.08 -1 72.04 72.08 72.02 3"

==== Accessing Analytics Data over the RESTful API

Spring XD has a discoverable RESTful API based on the Spring HATEAOS library.  You can discover the resources available by making a GET request on the root resource of the Admin server.  Here is an example where navigate down to find the data for a counter named 'httptap' that was created by these commands


[source,sh]
----
xd:>stream create --name httpStream --definition "http | file" --deploy
xd:>stream create --name httptap --definition "tap:stream:httpStream > counter" --deploy
xd:>http post --target http://localhost:9000 --data "helloworld"
----

The root resource returns 
[source,sh]
----
xd:>! wget  -q -S -O - http://localhost:9393/
{
  "links":[
    {},
    {
      "rel":"jobs",
      "href":"http://localhost:9393/jobs"
    },
    {
      "rel":"modules",
      "href":"http://localhost:9393/modules"
    },
    {
      "rel":"runtime/modules",
      "href":"http://localhost:9393/runtime/modules"
    },
    {
      "rel":"runtime/containers",
      "href":"http://localhost:9393/runtime/containers"
    },
    {
      "rel":"counters",
      "href":"http://localhost:9393/metrics/counters"
    },
    {
      "rel":"field-value-counters",
      "href":"http://localhost:9393/metrics/field-value-counters"
    },
    {
      "rel":"aggregate-counters",
      "href":"http://localhost:9393/metrics/aggregate-counters"
    },
    {
      "rel":"gauges",
      "href":"http://localhost:9393/metrics/gauges"
    },
    {
      "rel":"rich-gauges",
      "href":"http://localhost:9393/metrics/rich-gauges"
    }
  ]
}
----

Following the resource location for the counter

[source,sh]
----
xd:>! wget  -q -S -O - http://localhost:9393/metrics/counters
{
  "links":[

  ],
  "content":[
    {
      "links":[
        {
          "rel":"self",
          "href":"http://localhost:9393/metrics/counters/httptap"
        }
      ],
      "name":"httptap"
    }
  ],
  "page":{
    "size":0,
    "totalElements":1,
    "totalPages":1,
    "number":0
  }
}
----

And then the data for the counter itself
[source,sh]
----
xd:>! wget  -q -S -O - http://localhost:9393/metrics/counters/httptap
{
  "links":[
    {
      "rel":"self",
      "href":"http://localhost:9393/metrics/counters/httptap"
    }
  ],
  "name":"httptap",
  "value":2
}
----
