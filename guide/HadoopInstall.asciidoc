Installing Hadoop
-----------------

If you don't have a local Hadoop cluster available already, you can do a local http://hadoop.apache.org/docs/r1.1.2/single_node_setup.html[single node installation] and use that to try out Hadoop with Spring XD. The examples have been run with Hadoop 1.1.2, the stable release at the time of writing.

First, http://www.apache.org/dyn/closer.cgi/hadoop/common/:[download an installation archive] and unpack it locally. Linux users can also install Hadoop through the system package manager and on Mac OS X, you can use homebrew, but the installation is self-contained and it's easier to see what's going on if you just unpack it to a known location.

Change into the directory and have a look around

  $ cd hadoop-1.1.2
  $ ls
  $ bin/hadoop
  Usage: hadoop [--config confdir] COMMAND
  where COMMAND is one of:
    namenode -format     format the DFS filesystem
    secondarynamenode    run the DFS secondary namenode
    namenode             run the DFS namenode
    ...

The `bin` directory contains the start and stop scripts as well as the `hadoop` script which allows us to interact with hadoop from the command line. The next place to look is the `conf` directory. Following the Hadoop installation guide, edit the files in there for use in a http://hadoop.apache.org/docs/r1.1.2/single_node_setup.html#PseudoDistributed[Pseudo-Distributed Operation] configuration. Use the same ports given in that configuration. Our examples assume the HDFS daemon is running on port 9000.

Next make sure that you set `JAVA_HOME` in the `conf/hadoop-env.sh` script, or you will get an error when you start Hadoop. For example

  # The java implementation to use.  Required.
  # export JAVA_HOME=/usr/lib/j2sdk1.5-sun
  
  export JAVE_HOME/Library/Java/Home

As described in the installation guide, you also need to set up SSH login to locahost without a passphrase. On Linux, you may need to install the `ssh` package and ensure the `sshd` daemon is running. On Mac OS X, ssh is already installed but the `sshd` daemon isn't usually running. To start it, you need to enable "Remote Login" in the "Sharing" section of the control panel. Then you can carry on and setup SSH keys as described in the installation guide. Make sure you can log in at the command line using `ssh localhost` before trying to start hadoop:

  $ ssh localhost
  Last login: Thu May 30 12:52:47 2013

You also need to decide where in your local filesystem you want Hadoop to store its data. Let's say you decide to use `/data`.

First create the directory and make sure it is writeable:

  $ mkdir /data
  $ chmod 777 /data

Then edit `conf/core-site.xml` again to add the following property

   <property>
       <name>hadoop.tmp.dir</name>
       <value>/data</value>
   </property>


You're then ready to format the filesystem for use by HDFS

  $ bin/hadoop namenode -format

Running Hadoop
--------------

You should now finally be ready to run hadoop. Run the `start-all.sh` script 

  $ bin/start-all.sh

You should see five Hadoop Java processes running:

  $ jps
  4039 TaskTracker
  3713 NameNode
  3802 DataNode
  3954 JobTracker
  3889 SecondaryNameNode
  4061 Jps 

Try a few commands with `hadoop dfs` to make sure the basic system works

  $ bin/hadoop dfs -ls /
  Found 1 items
  drwxr-xr-x   - luke supergroup          0 2013-05-30 17:28 /data
  
  $ bin/hadoop dfs -mkdir /test
  $ bin/hadoop dfs -ls /
  Found 2 items
  drwxr-xr-x   - luke supergroup          0 2013-05-30 17:28 /data
  drwxr-xr-x   - luke supergroup          0 2013-05-30 17:31 /test
  
  $ bin/hadoop dfs -rmr /test
  Deleted hdfs://localhost:9000/test
 
At this point you should be good to create a Spring XD link:Streams[stream] using a Hadoop link:Sinks[sink].
 

  
