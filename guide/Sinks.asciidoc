=== Introduction
In this section we will show some variations on output sinks.  As a prerequisite start the XD Container
as instructed in the link:Getting-Started#getting-started[Getting Started] page.

The Sinks covered are

* <<log_sinks, Log>>

* <<file_sinks, File>>

* <<hdfs, HDFS>>

* <<avro, Avro>>

* <<jdbc_sink, JDBC>>

* <<tcp_sinks, TCP>>

* <<mail_sink, Mail>>

* <<rabbit_sink, RabbitMQ>>

* <<gemfire,GemFire Server>>

* <<splunk,Splunk Server>>

* <<mqtt_sink, MQTT>>

* <<router_sink, Dynamic Router>>

See the section link:Creating-a-Sink-Module#creating-a-sink-module[Creating a Sink Module] for information on how to create sink modules using other Spring Integration Adapters.

[[log_sinks]]
=== Log

Probably the simplest option for a sink is just to log the data. The `log` sink uses the application logger to output the data for inspection. The log level is set to `WARN` and the logger name is created from the stream name. To create a stream using a `log` sink you would use a command like

  xd:> stream create --name mylogstream --definition "http --port=8000 | log"

You can then try adding some data. We've used the `http` source on port 8000 here, so run the following command to send a message

  xd:> http post --target http://localhost:8000 --data "hello"

and you should see the following output in the XD container console.

  13/06/07 16:12:18 WARN logger.mylogstream: hello

The logger name is the sink name prefixed with the string "logger.". The sink name is the same as the stream name by default, but you can set it by passing the `--name` parameter 

  xd:> stream create --name myotherlogstream --definition "http --port=8001 | log --name=mylogger"

[[file_sinks]]
=== File Sink

Another simple option is to stream data to a file on the host OS. This can be done using the `file` sink module to create a link:Streams#streams[stream].

  xd:> stream create --name myfilestream --definition "http --port=8000 | file"

We've used the `http` source again, so run the following command to send a message

  xd:> http post --target http://localhost:8000 --data "hello"

The `file` sink uses the stream name as the default name for the file it creates, and places the file in the `/tmp/xd/output/` directory.

  $ less /tmp/xd/output/myfilestream
  hello

You can cutomize the behavior and specify the `name` and `dir` properties of the output file. For example

  xd:> stream create --name otherfilestream --definition "http --port=8000 | file --name=myfile --dir=/some/custom/directory"

==== File with Options

The file sink, by default, will add a newline at the end of each line; the actual newline will depend on the operating system.

This can be disabled by using `--binary=true`.

[[hdfs]]
=== Hadoop (HDFS)


If you do not have Hadoop installed, you can install Hadoop 1.2.1 as described in our link:Hadoop-Installation#installing-hadoop[separate guide]. Spring XD supports 4 Hadoop distributions, see link:Running-Distributed-Mode#using-hadoop[using Hadoop] for more information on how to start Spring XD to target a specific distribution.

Once Hadoop is up and running, you can then use the `hdfs` sink when creating a link:Streams#streams[stream]

  xd:> stream create --name myhdfsstream1 --definition "time | hdfs"

In the above example, we've scheduled `time` source to automatically send ticks to `hdfs` once in every second. If you wait a little while for data to accumuluate you can then list can then list the files in the hadoop filesystem using the shell's built in hadoop fs commands.  Before making any access to HDFS in the shell you first need to configure the shell to point to your name node.  This is done using the `hadoop config` command.

    xd:>hadoop config fs --namenode hdfs://localhost:8020

In this example the hdfs protocol is used but you may also use the webhdfs protocol.  Listing the contents in the output directory (named by default after the stream name) is done by issuing the following command.

  xd:>hadoop fs ls /xd/myhdfsstream1
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup          0 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt.tmp

While the file is being written to it will have the `tmp` suffix.  When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the `tmp` suffix.  There are several options to control the in use file file naming options.  These are `--inUsePrefix` and `--inUseSuffix` set the file name prefix and suffix respectfully.  

When you destroy a stream

  xd:>stream destroy --name myhdfsstream1

and list the stream directory again, in use file suffix doesn't exist anymore.

  xd:>hadoop fs ls /xd/myhdfsstream1
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup        380 2013-12-18 18:10 /xd/myhdfsstream1/myhdfsstream1-0.txt

To list the list the contents of a file directly from a shell execute the hadoop cat command.

  xd:>hadoop fs cat /xd/myhdfsstream1/myhdfsstream1-0.txt
  2013-12-18 18:10:07
  2013-12-18 18:10:08
  2013-12-18 18:10:09
  ...

In the above examples we didn't yet go through why the file was written in a specific directory and why it was named in this specific way. Default location of a file is defined as `/xd/<stream name>/<stream name>-<rolling part>.txt`. These can be changed using options `--directory` and `--fileName` respectively. Example is shown below. 

  xd:>stream create --name myhdfsstream2 --definition "time | hdfs --directory=/xd/tmp --fileName=data"
  xd:>stream destroy --name myhdfsstream2
  xd:>hadoop fs ls /xd/tmp
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup        120 2013-12-18 18:31 /xd/tmp/data-0.txt

It is also possible to control the size of a files written into HDFS. The `--rollover` option can be used to control when file currently being written is rolled over and a new file opened by providing the rollover size in bytes, kilobytes, megatypes, gigabytes, and terabytes.
 
  xd:>stream create --name myhdfsstream3 --definition "time | hdfs --rollover=100"
  xd:>stream destroy --name myhdfsstream3
  xd:>hadoop fs ls /xd/myhdfsstream3
  Found 3 items
  -rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-0.txt
  -rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-1.txt
  -rw-r--r--   3 jvalkealahti supergroup        100 2013-12-18 18:41 /xd/myhdfsstream3/myhdfsstream3-2.txt

Shortcuts to specify sizes other than bytes are written as `--rollover=64M`, `--rollover=512G` or `--rollover=1T`.

The stream can also be compressed during the write operation. Example of this is shown below.

  xd:>stream create --name myhdfsstream4 --definition "time | hdfs --codec=gzip"
  xd:>stream destroy --name myhdfsstream4
  xd:>hadoop fs ls /xd/myhdfsstream4
  Found 1 items
  -rw-r--r--   3 jvalkealahti supergroup         80 2013-12-18 18:48 /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip

From a native os shell we can use hadoop's fs commands and pipe data into gunzip. 

  # bin/hadoop fs -cat /xd/myhdfsstream4/myhdfsstream4-0.txt.gzip | gunzip
  2013-12-18 18:48:10
  2013-12-18 18:48:11
  ...

Often a stream of data may not have a high enough rate to roll over files frequently, leaving the file in an opened state.  This prevents users from reading a consistent set of data when running mapreduce jobs.  While one can alleviate this problem by using a small rollover value, a better way is to use the `idleTimeout`  option that will automatically close the file if there was no writes during the specified period of time.   This feature is also useful in cases where burst of data is written into a stream and you'd like that data to become visible in HDFS.

  xd:> stream create --name myhdfsstream5 --definition "http --port=8000 | hdfs --rollover=20 --idleTimeout=10000"

In the above example we changed a source to `http` order to control what we write into a `hdfs` sink. We defined a small rollover size and a timeout of 10 seconds. Now we can simply post data into this stream via source end point using a below command.

  xd:> http post --target http://localhost:8000 --data "hello"

If we repeat the command very quickly and then wait for the timeout we should be able to see that some files are closed before rollover size was met and some were simply rolled because of a rollover size.

  xd:>hadoop fs ls /xd/myhdfsstream5
  Found 4 items
  -rw-r--r--   3 jvalkealahti supergroup         12 2013-12-18 19:02 /xd/myhdfsstream5/myhdfsstream5-0.txt
  -rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-1.txt
  -rw-r--r--   3 jvalkealahti supergroup         24 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-2.txt
  -rw-r--r--   3 jvalkealahti supergroup         18 2013-12-18 19:03 /xd/myhdfsstream5/myhdfsstream5-3.txt


==== HDFS with Options

The HDFS Sink has the following options:

directory:: Where to output the files in the Hadoop FileSystem *(default: `/xd/<streamname>`)*
fileName:: The base filename to use for the created files (a counter will be appended before the file extension). *(default: `<streamname>`)*
fileExtension:: The file extension to use *(default: `txt`)*
rollover:: When to roll files over, expressed in bytes. Option can also expressed with a pattern as, `1M`, `1G`, `512G`, `1T` *(default: `1G`)*
codec:: If compression is used for stream. Possible values are `gzip`, `snappy`, `bzip2`, `lzo`. *(default: `no compression`)* 
idleTimeout::  Idle timeout in millis when Hadoop file resource is automatically closed. *(default: `0`, no timeout)*
inUseSuffix:: Temporary file suffix indicating that file is currently written and in use. *(default: `.tmp`)*
inUsePrefix:: Temporary file prefix indicating that file is currently written and in use. *(default: `none`)*
overwrite:: Flag indicating if file resources in Hadoop is allowed to be overwritten. *(default: `false`)*

[[avro]]
=== Hadoop Dataset (Avro)

The Hadoop Dataset Avro sink is used to store Java classes that are sent as the payload on the stream. It uses the http://kitesdk.org/[Kite SDK Data Module]'s Dataset implementation to store the payload data serialized in Avro format. The Avro schema is generated from the Java class that is persisted.

The Hadoop Dataset Avro sink requires that you have a Hadoop installation that is based on Hadoop v2 (Hadoop 2.2.0, Pivotal HD 1.0, Cloudera CDH4 or Hortonworks HDP 2.0), see link:Running-Distributed-Mode#using-hadoop[using Hadoop] for more information on how to start Spring XD to target a specific distribution.

Once Hadoop is up and running, you can then use the `avro` sink when creating a link:Streams#streams[stream]

  xd:>stream create --name mydataset --definition "time | avro --batchSize=20"

In the above example, we've scheduled `time` source to automatically send ticks to the `avro` sink once every second. The data will be stored in a directory named `/xd/<streamname>` by default, so in this example it will be `/xd/mydataset`. You can change this by supplying a `‑‑directory` parameter. The Avro data files are stored in a sub-directory named after the payload Java class, in this example the stream payload is a String so the name of the data sub-directory is `string`. If you have multiple Java classes as payloads, each class will get its own sub-directory.

Let the stream run for a minute or so. You can then list the contents of the hadoop filesystem using the shell's built in hadoop fs commands. You will first need to configure the shell to point to your name node using the hadoop config command. We use the hdfs protocol is to access the hadoop name node.

    xd:>hadoop config fs --namenode hdfs://localhost:8020

Then list the contents of the stream's data directory.

  xd:>hadoop fs ls /xd/mydataset/string
  Found 3 items
  drwxr-xr-x   - trisberg supergroup          0 2013-12-19 12:23 /xd/mydataset/string/.metadata
  -rw-r--r--   3 trisberg supergroup        202 2013-12-19 12:23 /xd/mydataset/string/1387473825754-63.avro
  -rw-r--r--   3 trisberg supergroup        216 2013-12-19 12:24 /xd/mydataset/string/1387473846708-80.avro

You can see that the sink has created two files containing the first two batches of 20 stream payloads each. There is also a `.metadata` directory created that contains the metadata that the Kite SDK Dataset implementation uses as well as the generated Avro schema for the persisted type. 

  xd:>hadoop fs ls /xd/mydataset/string/.metadata
  Found 2 items
  -rw-r--r--   3 trisberg supergroup        136 2013-12-19 12:23 /xd/mydataset/string/.metadata/descriptor.properties
  -rw-r--r--   3 trisberg supergroup          8 2013-12-19 12:23 /xd/mydataset/string/.metadata/schema.avsc


Now destroy the stream. 

  xd:>stream destroy --name mydataset

==== Avro with Options

The Avro Sink has the following options:

batchSize:: The number of payload objects that will be stored in each write operation. *(default: `10000`)*
directory:: Where the files will be written in the Hadoop FileSystem *(default: `/xd/<streamname>`)*
idleTimeout::  Idle timeout in milliseconds for when the aggregated batch of payload objects will be written even if the batchSize has not been reached. *(default: `-1`, no timeout)*

[[jdbc_sink]]
=== JDBC

The JDBC sink can be used to insert message payload data into a relational database table. By default it inserts the entire payload into a table named after the stream name in the HSQLDB database that XD uses to store metadata for batch jobs.  To alter this behavior you should modify the 'config/jdbc.properties' file with the connection parameters you want to use. There is also a 'config/init_db.sql' file that contains the SQL statements used to initialize the database table. You can modify this file if you'd like to create a table with your specific layout when the sink starts. You should also change the 'initializeDatabase' property to 'true' to have this script execute when the sink starts up.

The payload data will be inserted as-is if the 'columns' option is set to 'payload'. This is the default behavior.  If you specify any other column names the payload data will be assumed to be a JSON document that will be converted to a hash map. This hash map will be used to populate the data values for the SQL insert statement. A matching of column names with underscores like 'user_name' will match onto camel case style keys like 'userName' in the hash map.  There will be one insert statement executed for each message.

To create a stream using a `jdbc` sink relying on all defaults you would use a command like

  xd:> stream create --name mydata --definition "time | jdbc --initializeDatabase=true"

This will insert the time messages into a 'payload' column in a table named 'myjdbc'. Since the default is using the XD batch metadata HSQLDB database we can connect to this database instance from an external tool. After we let the stream run for a little while, we can connect to the database and look at the data stored in the database.

You can query the database with your favorite SQL tool using the following database URL: `jdbc:hsqldb:hsql://localhost:9101/xdjob` with `sa` as the user name and a blank password. You can also use the HSQL provided SQL Tool (download from link:http://hsqldb.org/[HSQLDB]) to run a quick query from the command line:
 
  $ java -cp ~/Downloads/hsqldb-2.3.0/hsqldb/lib/sqltool.jar org.hsqldb.cmdline.SqlTool --inlineRc url=jdbc:hsqldb:hsql://localhost:9101/xdjob,user=sa,password= --sql "select payload from mydata;"

This should result in something similar to the following output:

----
2014-01-06 09:33:25
2014-01-06 09:33:26
2014-01-06 09:33:27
2014-01-06 09:33:28
2014-01-06 09:33:29
2014-01-06 09:33:30
2014-01-06 09:33:31
2014-01-06 09:33:32
2014-01-06 09:33:33
2014-01-06 09:33:34
2014-01-06 09:33:35
2014-01-06 09:33:36
2014-01-06 09:33:37
----

Now we can destroy the stream using:

  xd:> stream destroy --name mydata

==== JDBC with Options

The JDBC Sink has the following options:

configProperties:: base name of properties file (in the config directory) containing configuration options for the sink. This file should contain the usual JDBC properties - driverClass, url, username, password *(default: `jdbc`)*
initializeDatabase:: whether to initialize the database using the initializer script (the default property file jdbc.properties has this set to true) *(default: `false`)*
initializerScript:: the file name for the script containing SQL statements used to initialize the database when the sink starts (will search config directory for this file) *(default: `init_db.sql`)*
tableName:: the name of the table to insert payload data into *(default: `<streamname>`)*
columns:: comma separated list of column names to include in the insert statement. Use 'payload' to include the entire message payload into a payload column. *(default: `payload`)*

[[tcp_sinks]]
=== TCP

The TCP Sink provides for outbound messaging over TCP.

The following examples use `netcat` (linux) to receive the data; the equivalent on Mac OSX is `nc`.

First, start a netcat to receive the data, and background it

     $ netcat -l 1234 &

Now, configure a stream

     xd:> stream create --name tcptest --definition "time --interval=3 | tcp"

This sends the time, every 3 seconds to the default tcp Sink, which connects to port `1234` on `localhost`.

----
$ Thu May 30 10:28:21 EDT 2013
Thu May 30 10:28:24 EDT 2013
Thu May 30 10:28:27 EDT 2013
Thu May 30 10:28:30 EDT 2013
Thu May 30 10:28:33 EDT 2013
----

TCP is a streaming protocol and some mechanism is needed to frame messages on the wire. A number of encoders are available, the default being 'CRLF'.

Destroy the stream; netcat will terminate when the TCP Sink disconnects.

    http://localhost:8080> stream destroy --name tcptest

==== TCP with Options

The TCP Sink has the following options

host:: the host (or IP Address) to connect to *(default: `localhost`)*
port:: the port on the `host` *(default `1234`)*
reverse-lookup:: perform a reverse DNS lookup on IP Addresses *(default: `false`)*
nio:: whether or not to use NIO *(default: `false`)*
encoder:: how to encode the stream  - see below *(default: `CRLF`)*
close:: whether to close the socket after each message *(default: `false`)*
charset:: the charset used when converting text from `String` to bytes *(default: `UTF-8`)*

Retry Options

retry-max-attempts:: the maximum number of attempts to send the data *(default: `5` - original request and 4 retries)*
retry-initial-interval:: the time (ms) to wait for the first retry *(default: `2000`)*
retry-multiplier:: the multiplier for exponential back off of retries *(default: `2`)*

With the default retry configuration, the attempts will be made after 0, 2, 4, 8, and 16 seconds.

==== Available Encoders

.Text Data

CRLF (default):: text terminated by carriage return (0x0d) followed by line feed (0x0a)
LF:: text terminated by line feed (0x0a)
NULL:: text terminated by a null byte (0x00)
STXETX:: text preceded by an STX (0x02) and terminated by an ETX (0x03)

.Text and Binary Data

RAW:: no structure - the client indicates a complete message by closing the socket
L1:: data preceded by a one byte (unsigned) length field (supports up to 255 bytes)
L2:: data preceded by a two byte (unsigned) length field (up to 2^16^-1 bytes)
L4:: data preceded by a four byte (signed) length field (up to 2^31^-1 bytes)


==== An Additional Example

Start netcat in the background and redirect the output to a file `foo`

     $ netcat -l 1235 > foo &

Create the stream, using the `L4` encoder

     xd:> stream create --name tcptest --definition "time --interval=3 | tcp --encoder=L4 --port=1235"

Destroy the stream

     http://localhost:8080> stream destroy --name tcptest

Check the output

----
$ hexdump -C foo
00000000  00 00 00 1c 54 68 75 20  4d 61 79 20 33 30 20 31  |....Thu May 30 1|
00000010  30 3a 34 37 3a 30 33 20  45 44 54 20 32 30 31 33  |0:47:03 EDT 2013|
00000020  00 00 00 1c 54 68 75 20  4d 61 79 20 33 30 20 31  |....Thu May 30 1|
00000030  30 3a 34 37 3a 30 36 20  45 44 54 20 32 30 31 33  |0:47:06 EDT 2013|
00000040  00 00 00 1c 54 68 75 20  4d 61 79 20 33 30 20 31  |....Thu May 30 1|
00000050  30 3a 34 37 3a 30 39 20  45 44 54 20 32 30 31 33  |0:47:09 EDT 2013|
----

Note the 4 byte length field preceding the data generated by the `L4` encoder.

[[mail_sink]]
=== Mail

The "mail" sink allows sending of messages as emails, leveraging Spring Integration mail-sending channel adapter. Please refer to Spring Integration documentation for the details, but in a nutshell, the sink is able to handle String, byte[] and MimeMessage messages out of the box.

Here is a simple example of how the mail module is used:

  xd:> stream create mystream --definition "http | mail --to='"your.email@gmail.com"' --host=your.imap.server --subject=payload+' world'"

Then,

  xd:> http post --data Hello 

You would then receive an email whose body contains "Hello" and whose subject is "Hellow world". Of special attention here is the way you need to escape strings for most of the parameters, because they're actually SpEL expressions (so here for example, we used a String literal for the `to` parameter).

The full list of options available to the mail module is below (note that most of these options can be set once and for all in the `mail.properties` file):

  to:: The primary recipient(s) of the email. *(default: `null`, SpEL Expression)*
  from:: The sender address of the email. *(default: `null`, SpEL Expression)*
  subject:: The email subject. *(default: `null`, SpEL Expression)*
  cc:: The recipient(s) that should receive a carbon copy. *(default: `null`, SpEL Expression)*
  bcc:: The recipient(s) that should receive a blind carbon copy. *(default: `null`, SpEL Expression)*
  replyTo:: The address that will become the recipient if the original recipient decides to "reply to" the email. *(default: `null`, SpEL Expression)*
  contentType:: The content type to use when sending the email. *(default: `null`, SpEL Expression)*
  host:: The hostname of the sending server to use. *(default: `localhost`)*
  port:: The port of the sending server. *(default: `25`)*
  username:: The username to use for authentication against the sending server. *(default: none)*
  password:: The password to use for authentication against the sending server. *(default: none)*
    


[[rabbit_sink]]
=== RabbitMQ

The "rabbit" sink enables outbound messaging over RabbitMQ.

The following example shows the default settings.

Configure a stream:

     xd:> stream create --name rabbittest --definition "time --interval=3 | rabbit"

This sends the time, every 3 seconds to the default (no-name) Exchange for a RabbitMQ broker running on localhost, port 5672.

The routing key will be the name of the stream by default; in this case: "rabbittest". Since the default Exchange is a direct-exchange to which all Queues are bound with the Queue name as the binding key, all messages sent via this sink will be passed to a Queue named "rabbittest", if one exists. We do not create that Queue automatically. However, you can easily create a Queue using the RabbitMQ web UI. Then, using that same UI, you can navigate to the "rabbittest" Queue and click the "Get Message(s)" button to pop messages off of that Queue (you can choose whether to requeue those messages).

To destroy the stream, enter the following at the shell prompt:

    xd:> stream destroy --name rabbittest

==== RabbitMQ with Options

The RabbitMQ Sink has the following options

username:: the username to connect to the RabbitMQ broker *(default: `guest` unless `spring.rabbitmq.username` has been overridden in `rabbit.properties`)*
password:: the password to connect to the RabbitMQ broker *(default: `guest` unless `spring.rabbitmq.password` has been overridden in `rabbit.properties`)*
host:: the host (or IP Address) to connect to *(default: `localhost` unless `spring.rabbitmq.host` has been overridden in `rabbit.properties`)*
port:: the port on the `host` *(default: `5672` unless `spring.rabbitmq.port` has been overridden in `rabbit.properties`)*
vhost:: the virtual host *(default: `/` unless `spring.rabbitmq.virtual_host` has been overridden in `rabbit.properties`)*
exchange:: the Exchange on the RabbitMQ broker to which messages should be sent *(default: `` (empty: therefore, the default no-name Exchange))*
routingKey:: the routing key to be passed with the message. Note: If the routing key is not passed with the message and simply be a string literal (like the queue name), please make sure to specify it as SpEL literal. *(default: <streamname>)*

Note: the `rabbit.properties` file referred to above is located within the `XD_HOME/config` directory.

[[gemfire]]
=== GemFire Server

Currently XD supports GemFire's client-server topology. A sink that writes data to a GemFire cache requires at least one  cache server to be running in a separate process and may also be configured to use a Locator. While Gemfire configuration is outside of the scope of this document, details are covered in the https://www.vmware.com/support/pubs/vfabric-gemfire.html[GemFire Product documentation]. The XD distribution includes a standalone GemFire server executable suitable for development and test purposes and bootstrapped using a Spring configuration file provided as a command line argument. The GemFire jar is distributed freely under GemFire's development license and is subject to the license's terms and conditions. Sink modules provided with the XD distrubution that write data to GemFire create a client cache and client region. No data is cached on the client.

==== Launching the XD GemFire Server

To start the GemFire cache server GemFire Server included in the Spring XD distribution, go to the XD install directory:

   $cd gemfire/bin
   $./gemfire-server ../config/cq-demo.xml

The command line argument is the path of a Spring Data Gemfire configuration file with including a configured cache server and one or more regions. A sample cache configuration is provided https://github.com/SpringSource/spring-xd/blob/master/spring-xd-gemfire-server/config/cq-demo.xml[cq-demo.xml] located in the `config` directory. Note that Spring interprets the path as a relative path unless it is explicitly preceded by `file:`. The sample configuration starts a server on port 40404 and creates a region named _Stocks_. 

==== Gemfire sinks

There are 2 implementation of the gemfire sink: _gemfire-server_ and _gemfire-json-server_. They are identical except the latter converts JSON string payloads to a JSON document format proprietary to GemFire and provides JSON field access and query capabilities. If you are not using JSON, the gemfire-server module will write the payload using java serialization to the configured region. Either of these modules accepts the following attributes:

regionName:: the name of the GemFire region. This must be the name of a region configured for the cache server. This module creates the corresponding client region. *(default: `<streamname>`)*
keyExpression:: A SpEL expression which is evaluated to create a cache key. Typically, the key value is derived from the payload. *(default: `<streamname>`*, which will overwrite the same entry for every message received on the stream)
gemfireHost:: The host name or IP address of the cache server or locator *(default: `localhost`)*
gemfirePort:: The TCP port number of the cache server or locator *(default: `40404`)*
useLocator:: A boolean flag indicating that the above host and port refer to a locator *(default: `false`)* 

NOTE: The locator option is mostly intended for integration with an existing GemFire installation in which the cache servers are configured to use locators in accordance with best practice. While GemFire supports configuration of multiple locators for failover, this is currently not supported in XD. However, using a single virtual IP backed by hardware routers for failover has proven to be an effective and simpler alternative. 

==== Example
Suppose we have a JSON document containing a stock price:

      {"symbol":"VMW", "price":73} 

We want this to be cached using the stock symbol as the key. The stream definition is:

     http | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')

The keyExpression is a SpEL expression that depends on the payload type. In this case, _com.gemstone.org.json.JSONObject. JSONObject_ which  provides the _getField_ method. To run this example:

    xd:> stream create --name stocks --definition "http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')"
    
    xd:> http post --target http://localhost:9090 --data "{"symbol":"VMW","price":73}"

This will write an entry to the GemFire _Stocks_ region with the key _VMW_.  Please do not put spaces when separating the JSON key-value pairs, only a comma. 

You should see a message on STDOUT for the process running the GemFire server like:

    INFO [LoggingCacheListener] - updated entry VMW

[[splunk]]
=== Splunk Server
A http://www.splunk.com/[Splunk] sink that writes data to a TCP Data Input type for Splunk. 

==== Splunk sinks
The Splunk sink converts an object payload to a string using the object’s toString method and then converts this to a SplunkEvent that is sent via TCP to Splunk.  The module accepts the following attributes:

host::
The host name or IP address of the Splunk server *(default: `localhost`)
port::
The TCP port number of the Splunk Server *(default: `8089`)*
username::
The login name that has rights to send data to the tcp-port *(default: `admin`)*
password::
The password associated with the username *(default: `password`)*
owner::
The owner of the tcp-port *(default: `admin1`)*
tcp-port::
The TCP port number to where XD will send the data *(default: `9500`)*

==== Setup Splunk for TCP Input
. From the Manager page select `Manage Inputs` link
. Click the `Add data` Button
. Click the `From a TCP port` link
. `TCP Port` enter the port you want Splunk to monitor
. `Set Source Type` select `Manual`
. `Source Type` enter `tcp-raw`
. Click `Save`

==== Example
An example stream would be to take data from a twitter search and push it through to a splunk instance.

    xd:> stream create --name springone2gx --definition "twittersearch --consumerKey= --consumerSecret= --query='#LOTR' | splunk"

[[mqtt_sink]]
=== MQTT
The mqtt sink connects to an mqtt server and publishes telemetry messages.

==== Options

The folllowing options are configured in mqtt.properties in XD_HOME/config

    mqtt.url=tcp://localhost:1883
    mqtt.default.client.id=xd.mqtt.client.id
    mqtt.username=guest
    mqtt.password=guest
    mqtt.default.topic=xd.mqtt.test

The defaults are set up to connect to the RabbitMQ MQTT adapter on localhost.

Note that the client id must be no more than 19 characters; this is because `.snk` is added and the id must be no more than 23 characters.

clientId:: Identifies the client - overrides the default above.
topic:: The topic to which the sink will publish - overrides the default above.
qos:: The Quality of Service (default: 1)
retained:: Whether the retained flag is set (default: false)

[[router_sink]]
=== Dynamic Router

The Dynamic Router support allows for routing Spring XD messages to *named channels* based on the evaluation of SpEL expressions or Groovy Scripts.

==== SpEL-based Routing

In the following example, 2 streams are created that listen for message on the *foo* and the *bar* channel. Furthermore, we create a stream that receives messages via HTTP and then delegates the received messages to a router:

----
xd:>stream create f --definition "queue:foo > transform --expression=payload+'-foo' | log"
Created new stream 'f'

xd:>stream create b --definition "queue:bar > transform --expression=payload+'-bar' | log"
Created new stream 'b'

xd:>stream create r --definition "http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'"
Created new stream 'r'
----

Now we make 2 requests to the HTTP source:

----
xd:>http post --data "a"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 a
> 200 OK

xd:>http post --data "b"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 b
> 200 OK
----

In the server log you should see the following output:

----
11:54:19,868  WARN ThreadPoolTaskScheduler-1 logger.f:145 - a-foo
11:54:25,669  WARN ThreadPoolTaskScheduler-1 logger.b:145 - b-bar
----

For more information, please also consult the Spring Integration Reference manual: http://static.springsource.org/spring-integration/reference/html/messaging-routing-chapter.html#router-namespace particularly the section "Routers and the Spring Expression Language (SpEL)".	

==== Groovy-based Routing

Instead of SpEL expressions, Groovy scripts can also be used. Let's create a Groovy script in the file system at "/my/path/router.groovy"

[source,groovy]
----
println("Groovy processing payload '" + payload +"'");
if (payload.contains('a')) {
	return ":foo"
}
else {
	return ":bar"
}
----

Now we create the following streams:

----
xd:>stream create f --definition ":foo > transform --expression=payload+'-foo' | log"
Created new stream 'f'

xd:>stream create b --definition ":bar > transform --expression=payload+'-bar' | log"
Created new stream 'b'

xd:>stream create g --definition "http | router --script='file:/my/path/router.groovy'"
----

Now post some data to the HTTP source:

----
xd:>http post --data "a"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 a
> 200 OK

xd:>http post --data "b"
> POST (text/plain;Charset=UTF-8) http://localhost:9000 b
> 200 OK
----

In the server log you should see the following output:

----
Groovy processing payload 'a'
11:29:27,274  WARN ThreadPoolTaskScheduler-1 logger.f:145 - a-foo
Groovy processing payload 'b'
11:34:09,797  WARN ThreadPoolTaskScheduler-1 logger.b:145 - b-bar
----

[NOTE]
===============================
You can also use Groovy scripts located on your classpath by specifying:
----
--script='org/my/package/router.groovy'
----
===============================

For more information, please also consult the Spring Integration Reference manual: "Groovy support"
http://static.springsource.org/spring-integration/reference/html/messaging-endpoints-chapter.html#groovy

==== Options

expression:: The SpEL expression to use for routing
script:: Indicates that Groovy Script based routing is used. If this property is set, then the "Expression" attribute will be ignored. The groovy script is checked for updates every 60 seconds. The script can be loaded from the classpath or from the file system e.g. "--script='org/springframework/springxd/samples/batch/router.groovy'" or "--script='file:/my/path/router.groovy'"
properties-location:: Will be made available as script variables for Groovy Script based routing. Will only be evaluated once at initialization time. By default the following script variables will be made available: "payload" and "headers".