=== Introduction
One of the features that XD offers is the ability to launch and monitor batch jobs based on http://www.springsource.org/spring-batch[Spring Batch].  The Spring Batch project was started in 2007 as a collaboration between SpringSource and Accenture to provide a comprehensive framework to support the development of robust batch applications.  Batch jobs have their own set of best practices and domain concepts which have been incorporated into Spring Batch building upon Accenture's consulting business.  Since then Spring Batch has been used in thousands of enterprise applications and is the basis for the recent JSR standardization of batch processing, https://jcp.org/en/jsr/detail?id=352[JSR-352].

Spring XD builds upon Spring Batch to simplify creating batch workflow solutions that span traditional use-cases such as moving data between flat files and relational databases as well as Hadoop use-cases where analysis logic is broken up into several steps that run on a Hadoop cluster.  Steps specific to Hadoop in a workflow can be MapReduce jobs, executing Hive/Pig scripts or HDFS operations.

=== Workflow
The concept of a workflow translates to a Job, not to be confused with a MapReduce job. A Job is a directed graph, each node of the graph is a processing Step. Steps can be executed sequentially or in parallel, depending on the configuration. Jobs can be started, stopped, and restarted. Restarting
jobs is possible since the progress of executed steps in a Job is persisted in a database via a JobRepository.  The following figures shows the basic components of a workflow.

image::images/batch-overview.png[The Spring XD Workflow overview, scaledwidth="75%"]

A Job that has steps specific to Hadoop is shown below.

image::images/batch-hadoop-overview.png[Steps in a workflow that execute Hadoop HDFS operations and run Pig, MapReduce and Hive jobs, scaledwidth="75%"]

A JobLauncher is responsible for starting a job and is often triggered via a scheduler.  Other options to launch a job are through Spring XD's RESTful administration API, the XD web application, or in response to an external event from and XD stream defintion, e.g file polling using the file source.

=== Features

Spring XD allows you to create and launch jobs.  The launching of a job can be trigger using a cron expression or in reaction to data on a stream. When jobs are executing, they are also a souce of event data that can be subscribed to by a stream.  There are several type of events sent during a job's execution, the most common being the status of the job and the steps taken within the job.  This bi-direction communication between stream processing and batch processing allows for more complex chains of processing to be developed.

As a starting point, jobs for the following cases are provided to use out of the box

* Poll a Directory and import CSV files to HDFS
* Import CSV files to JDBC
* HDFS to JDBC Export
* JDBC to HDFS Import
* HDFS to MongoDB Export

These are described in the section below.  

This purpose of this section is to show you how to create, schedule and monitor a job.

=== Developing your Job

The Jobs definitions provided as part of the Spring XD distribution as well as those included in the https://github.com/spring-projects/spring-xd-samples[Spring XD Samples] repository can be used a basis for building your own custom Jobs.  The development of a Job largely follows the development of a Spring Batch job, for which there are several references.

* http://projects.spring.io/spring-batch/[Spring Batch home page]
* http://www.manning.com/templier/[Spring Batch In Action - Manning]
* http://www.apress.com/9781430234524[Pro Spring Batch - APress]

For help developing Job steps specific to Hadoop, e.g. HDFS, Pig, Hive, the https://github.com/spring-projects/spring-xd-samples[Spring XD Samples] is useful as wella s the following resources

* http://projects.spring.io/spring-hadoop/[Spring for Apache Hadoop home page]
* http://shop.oreilly.com/product/0636920024767.do[Spring Data - O'Reilly - Chapter 13]

Once your Jobs have been developed and unit tested, they are integrated into Spring XD by copying the resulting .jar file and Job XML definition to $XD_HOME/lib and $XD_HOME/modules/jobs.

=== Creating a Job

To describe the creation of a job we will use the job definition that is part of the https://github.com/spring-projects/spring-xd-samples/tree/master/batch-simple[batch-simple example].

To create a job in the XD shell, execute the job create command composed of:

* name - the "name" that will be associated with the Job
* definition - the name of the context file that describes the tasklet.

So using our example above where we have a myjob.xml job definiton file in the $XD_HOME/modules/jobs directory, this will look like:
----
xd:> job create --name helloSpringXD --definition "myjob"
----
In the logging output of the XDContainer you should see the following:
----
14:17:46,793  INFO http-bio-8080-exec-5 job.JobPlugin:87 - Configuring module with the following properties: {numberFormat=, dateFormat=, makeUnique=true, xd.stream.name=helloSpringXD}
14:17:46,837  INFO http-bio-8080-exec-5 module.SimpleModule:140 - initialized module: SimpleModule [name=myjob, type=job, group=helloSpringXD, index=0]
14:17:46,840  INFO http-bio-8080-exec-5 module.SimpleModule:154 - started module: SimpleModule [name=job, type=job, group=helloSpringXD, index=0]
14:17:46,840  INFO http-bio-8080-exec-5 module.ModuleDeployer:152 - launched job module: helloSpringXD:myjob:0
----

==== Creating Jobs - Additional Options

When creating jobs, the following options are available:

deploy:: If true, depploys the stream immediately *(default: `true`)*
dateFormat:: The optional date format for job parameters *(default: `yyyy/MM/dd`)*
numberFormat:: Defines the numberformat when parsing numeric parameters *(default: `NumberFormat.getInstance(Locale.US)`)*
makeUnique:: Shall job parameters be made unique? *(default: `true`)*

=== Launching a job
XD uses triggers as well as regular event flow to launch the batch jobs.  So in this section we will cover how to:

* Launch the Batch Job Ad-hoc
* Launch the Batch Job using a named Cron-Trigger
* Launch the Batch Job as sink.

==== Ad-hoc
To launch a job one time, use the launch option of the job command.  So going back to our example above, we've created a job module instance named helloSpringXD.  Launching that Job Module Instance would look like:
----
xd:> job launch helloSpringXD
----
In the logging output of the XDContainer you should see the following
----
16:45:40,127  INFO http-bio-9393-exec-1 job.JobPlugin:98 - Configuring module with the following properties: {numberFormat=, dateFormat=, makeUnique=true, xd.stream.name=myjob}
16:45:40,185  INFO http-bio-9393-exec-1 module.SimpleModule:140 - initialized module: SimpleModule [name=job, type=job, group=myjob, index=0 @3a9ecb9d]
16:45:40,198  INFO http-bio-9393-exec-1 module.SimpleModule:161 - started module: SimpleModule [name=job, type=job, group=myjob, index=0 @3a9ecb9d]
16:45:40,199  INFO http-bio-9393-exec-1 module.ModuleDeployer:161 - deployed SimpleModule [name=job, type=job, group=myjob, index=0 @3a9ecb9d]
Hello Spring XD!
----
To re-launch the job just execute the launch command.
For example:
----
xd:> job launch helloSpringXD
----
==== Launch the Batch using Cron-Trigger
To launch a batch job based on a cron scheduler is done by creating a stream using the trigger source.

----
xd:> stream create --name cronStream --definition "trigger --cron='0/5 * * * * *'  > queue:job:myCronJob"

----
A batch job can receive parameters from a source (in this case a trigger) or process. A trigger uses the --payload expression to declare its payload.
----
xd:> stream create --name cronStream --definition "trigger --cron='0/5 * * * * *'  --payload='{"param1":"Kenny"}' > queue:job:myCronJob"
----
NOTE: The payload content must be in a JSON-based map representation.

To pause/stop future scheduled jobs from running for this stream, the stream must be undeployed for example:
----
xd:> stream undeploy --name cronStream
----
==== Launch the Batch using a Fixed-Delay-Trigger
A fixed-delay-trigger is used to launch a Job on a regular interval.  Using the --fixedDelay parameter you can set up the number of seconds between executions.  In the example below we are running myXDJob every 10 seconds and passing it a payload containing a single attribute.
----
xd:> stream create --name fdStream --definition "trigger --payload='{"param1":"fixedDelayKenny"}' --fixedDelay=10 > queue:job:myXDJob"
----
To pause/stop future scheduled jobs from running for this stream, you must undeploy the stream for example:
----
xd:> stream undeploy --name cronStream
----
==== Launch job as a part of event flow
A batch job is always used as a sink, with that being said it can receive messages from sources (other than triggers) and processors. In the case below we see that the user has created a http source (http source receives http posts and passes the payload of the http message to the next module in the stream) that will pass the http payload to the "myHttpJob".

----
 stream create --name jobStream --definition "http > queue:job:myHttpJob"
----
To test the stream you can execute a http post, like the following:
----
xd:> http post --target http://localhost:9000 --data "{"param1":"fixedDelayKenny"}"
----
=== Retrieve job notifications
XD offers the facilities to capture the notifications that are sent from the job as it is executing.

Notifications include:

* Job Execution Listener
* Chunk Listener
* Item Listener
* Step Execution Listener
* Skip Listener

In this example, the job will send notifications to the log.
----
stream create --name jobNotifications --definition ":myHttpJob-notifications >log"
----
In the logging output of the container you should see something like the following when the job completes:
----
15:26:30,029  WARN task-scheduler-5 logger.jobNotifications:145 - JobExecution: id=1, version=2, startTime=Wed Aug 28 15:26:30 EDT 2013, endTime=Wed Aug 28 15:26:30 EDT 2013, lastUpdated=Wed Aug 28 15:26:30 EDT 2013, status=COMPLETED, exitStatus=exitCode=COMPLETED;exitDescription=, job=[JobInstance: id=1, version=0, Job=[myHttpJob.job]], jobParameters=[{random=0.49881213192780494}]
----

=== Removing Batch Jobs

Batch Jobs can be deleted by executing:

----
xd:> job destroy helloSpringXD
----

Alternatively, one can just undeploy the job, keeping its definition for a future redeployment:

----
xd:> job undeploy helloSpringXD
----


=== Pre-Packaged Batch Jobs

Spring XD comes with several batch import and export modules. You can run them out of the box or use them as a basis for building your own custom modules.

==== Poll a Directory and Import CSV Files to HDFS (`filepollhdfs`)

This module is designed to be driven by a stream polling a directory. It imports data from CSV files and requires that you supply a list of named columns for the data using the `names` parameter. For example:

----
xd:> job create myjob --definition "filepollhdfs --names=forename,surname,address"
----

You would then use a stream with a file source to scan a directory for files and drive the job. A separate file will be started for each job found:

----
xd:> stream create csvStream --definition "file --ref=true --dir=/mycsvdir --pattern=*.csv > queue:job:myjob"
----

==== Import CSV Files to JDBC (`filejdbc`)

A module which loads CSV files into a JDBC table using a single batch job. By default it uses the file `config/batch-jdbc.properties` to configure the module and stores data in the internal HSQL DB which is used by Spring Batch. The job should be defined with the `resources` parameter defining the files which should be loaded. It also requires a `names` parameter (for the CSV field names) and these should match the database column names into which the data should be stored. You can either pre-create the database table or the module will create it for you if you use --initializeDatabase=true when the job is created. The table intitialization is configured in a similar way to the JDBC sink and uses the same parameters. The default table name is the job name and can be customized by setting the `tableName` parameter. As an example, if you run the command

----
xd:> job create myjob --definition "filejdbc --resources=file:///mycsvdir/*.csv --names=forename,surname,address --tableName=people --initializeDatabase=true"
----

it will create the table "people" in the database with three varchar columns called "forename", "surname" and "address". When you launch the job it will load the files matching the resources pattern and write the data to this table.

Launch the job using:

----
xd:> job launch myjob
----

==== HDFS to JDBC Export (`hdfsjdbc`)

This module functions very similarly to the `filejdbc` one except that the resources you specify should actually be in HDFS, rather than the OS filesystem. 

----
xd:> job create myjob --definition "hdfsjdbc --resources=/xd/data/*.csv --names=forename,surname,address --tableName=people --initializeDatabase=true"
----

Launch the job using:

----
xd:> job launch myjob
----

==== JDBC to HDFS Import (`jdbchdfs`)

Performs the reverse of the previous module. The database configuration is the same as for `filejdbc` but without the initialization options since you need to already have the data to import into HDFS. When creating the job, you must either supply the select statement by setting the `sql` parameter, or you can supply both `tableName` and `columns` options (which will be used to build the SQL statement).

To import data from the database table `some_table`, you could use

----
xd:> job create myjob --definition "jdbchdfs --sql='select col1,col2,col3 from some_table'"
----

You can customize how the data is written to HDFS by supplying the options `directory` (defaults to `/xd/(job name)`), `fileName` (defaults to job name), `rollover` (in bytes, default 1000000) and `fileExtension` (defaults to 'csv').

Launch the job using:

----
xd:> job launch myjob
----

==== HDFS to MongoDB Export (`hdfsmongodb`)

Exports CSV data from HDFS and stores it in a MongoDB collection which defaults to the stream name. This can be overridden with the `collectionName` parameter. The job is configured using the file `config/batch-mongo.properties`. Once again, the field names should be defined by supplying the `names` parameter. The data is converted internally to a Spring XD `Tuple` and the collection items will have an `id` matching the tuple's UUID. You can override this by setting the `idField` parameter to one of the field names if desired.

An example:

----
xd:> job create myjob --definition "hdfsmongodb --resources=/data/*.log --names=employeeId,forename,surname,address --idField=employeeId --collectionName=people"
----

