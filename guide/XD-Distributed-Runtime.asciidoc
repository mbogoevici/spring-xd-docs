=== Introduction
This document describes what's happening "under the hood" of the XD Distributed Runtime (DIRT) and, in particular, how the runtime architecture achieves high availability and failover in a clustered production environment. See link:Running-Distributed-Mode[Running in Distributed Mode] for more information on installing and running Spring XD in distributed mode.

This discussion will focus on the core runtime components and the role of ZooKeeper.

=== Configuring XD for High Availabilty (HA)

A production XD environment is typically distributed among multiple hosts in a clustered environment. XD scales horizontally by providing additional Container instances. In the simplest case, all containers are replicas, that is, they are interchangeable and a module may be deployed to any instance in a round-robin fashion. XD supports a flexible container matching algorithm to target modules to specific container configurations. The matching algorithm will be covered in more detail later, but for now, let's assume the simple case. Running multiple containers not only supports horizontal scalability, but allows for failover. If one container goes down, any modules deployed to that container will be deployed to another available instance.  

XD requires that a single active Admin server handle interactions with the containers, such as stream deployment requests, as these types of operations must be carefully coordinated and processed in the order received. Without a backup Admin server, this component becomes single point of failure. Therefore, two (or more for the risk averse) Admin servers are recommended for a production environment. Note that every Admin server can accept all requests via REST endpoints but only one instance, the "Leader", will actually perform requests that update the runtime state. If the Leader goes down, another available Admin server will assume the role.

An HA XD installation also requires that additional required servers - ZooKeeper, messaging middleware, and data stores listed above - be configured for HA as well. Please consult the product documentation for specific recommendations regarding these components.

==== HA Messsage Bus

The +RabbitMessageBus+ allows for HA configuration using normal https://www.rabbitmq.com/ha.html[RabbitMQ HA Configuration]. 

First, use the +addresses+ property in +servers.yml+ to include the host/port for each server in the cluster. See link:Application-Configuration#rabbitConfig[Application Configuration].

By default, queues and exchanges declared by the bus are prefixed with +xdbus.+ (this prefix can be changed as described in link:Application-Configuration#rabbitBusProps[Application Configuration]).

To configure the entire bus for HA, create a policy:

+rabbitmqctl set_policy ha-xdbus "^xdbus\." '{"ha-mode":"all"}'+


=== ZooKeeper Overview

In the previous section, we claimed that if a container goes down, XD will redeploy anything that is deployed on that instance to another available container. We also claimed that if the Admin Leader goes down, another Admin server will assume that role. ZooKeeper is what makes this all possible. ZooKeeper is a widely used Apache project designed primarily for cluster management and coordination. This section will cover some basic concepts necessary to understand its role in XD. See https://cwiki.apache.org/confluence/display/ZOOKEEPER/Index[The ZooKeeper Wiki] for a more complete overview.

ZooKeeper is based on a simple hierarchical data structure, formally a tree, but conceptually and semantically similar to a file directory structure. As such, data is stored in _nodes_. A node is referenced via a _path_, e.g., _/xd/streams/mystream_. Each node can store additional data, serialized as a byte array. In XD, all data is a java.util.Map serialized as JSON.

A node is created to be either _ephemeral_ or _persistent_.  An ephemeral node exists only as long as the process that created it. A persistent node is, well, persistent. For example, ephemeral nodes are appropriate for registering Container instances. When an XD container starts up, it registers itself as an ephemeral node, _/xd/containers/<container-id>_, where XD generates a unique container id. When the container goes down, its node is removed. Persistent nodes are used to manage state needed for recovery and failover that must be available independent of a Container instance. This includes data such as stream definitions, job definitions, deployment manifests, and module deployments.

Obviously ZooKeeper is critically important to the XD runtime and must itself be HA. ZooKeeper itself supports a clustered architecture, called an _ensemble_. The details are beyond the scope of this document, but for the sake of discussion, there should be at least three ZooKeeper server instances running (an odd number is always recommended). The XD Container and Admin nodes are clients to the ZooKeeper ensemble and must connect to ZooKeeper at startup. XD components are configured with a _zk.client.connect_ property which may designate a single <host>:<port> or a comma separated list. The ZooKeeper client will attempt to connect to each server in order until it succeeds. If it is unable to connect, it will keep trying. If a connection goes down, the ZooKeeper client will attempt to reconnect to one of the servers. The ZooKeeper cluster guarantees consistent replication of data across the ensemble. ZooKeeper maintains data primarily in memory backed by a disk cache. 

In addition to performing CRUD operations on nodes, A ZooKeeper client can register a callback on a node to respond to  any events or state changes to that node or any of its children. Such node operations and callbacks are the mechanism that control the XD runtime. 

image::images/xd-cluster.png[Spring XD and ZK Ensemble, width=500]

=== The Admin Server Internals

Assuming more than one Admin instance is running, Each instance requests leadership at start up. If there is already a designated leader, the instance will watch the _xd/admin_ node to be notified if the Leader goes away. The instance designated as the "Leader", using the Leader Selector recipe provided by http://curator.apache.org[Curator], a ZooKeeper client library that implements some common patterns. Curator also provides some Listener callback interfaces that the client can register on a node. The AdminServer creates the top level nodes for xd:

* */xd/admins* - children are ephemeral nodes for each available Admin instance and used for Leader Selector 
* */xd/containers* - children are ephemeral nodes containing runtime attributes for each available container
* */xd/streams* - children are persistent nodes containing the definition for each stream, however the leaf nodes for a deployed stream, at the module level, are ephemeral nodes added by the container to which the module is deployed. 
* */xd/jobs* - children are persistent nodes containing the definition for each job, however the leaf node for a deployed job is an ephemeral node added by the container to which the job is deployed.
* */xd/deployments/streams* - children are persistent nodes containing stream deployment status
* */xd/deployments/jobs* - children are persistent nodes containing job deployment status

and regiseters a LeaderListener which is used by the selected Leader. 

The Leader registers listeners on /xd/deployments/streams, /xd/deployments/jobs, and /xd/containers to handle events related to stream deployments, job deployments, and be notified when containers are added and removed from the cluster. Note that any Admin instance can handle user requests. For example, if you enter the following commands via XD shell,

----
xd>stream create ticktock --definition "time | log"
----  
This command will invoke a REST service on its connected Admin instance to create a new node /xd/streams/ticktock

----
xd>stream deploy ticktock
---- 

This will create a new node /xd/deployments/streams/ticktock 

If the Admin instance connected to the shell is not the Leader, it will perform no further action. The Leader listening to /xd/deployments/streams will respond to the newly added child node and deploy each module in the stream definition to a different Container, if possible, and update the runtime state accordingly.

image::images/xd-admin-internals.png[XD Admin Internals, width=500]

==== Example

Let's walk through a simple example. If you don't have an XD cluster set up, the basics can be illustrated by running XD in single node. From the XD install directory:

----
$export JAVA_OPTS="-Dzk.embedded.server.port=5555"
$xd/bin/xd-singlenode
----
XD single node runs with an embedded zookeeper server by default and will assign a random unused port. The _zk.embedded.server.port_ property will assign the requested port if available.


In another terminal session, start the ZooKeeper CLI included with ZooKeeper to connect to the embedded server and inspect the contents of the nodes (NOTE: tab completion works) :

----
$zkCli.sh -server localhost:5555
----
After some console output, you should see a prompt:

----
WatchedEvent state:SyncConnected type:None path:null
[zk: localhost:5555(CONNECTED) 0]
----
navigate using the _ls_ command: 

----
[[zk: localhost:5555(CONNECTED) 0] ls /xd
[containers, jobs, streams, admin, deployments]
[zk: localhost:5555(CONNECTED) 1] ls /xd/streams
[]
[zk: localhost:5555(CONNECTED) 2] ls /xd/deployments
[jobs, streams, modules]
[zk: localhost:5555(CONNECTED) 3] ls /xd/deployments/streams
[]
[zk: localhost:5555(CONNECTED) 4] ls /xd/deployments/modules
[2ebbbc9b-63ac-4da4-aa32-e39d69eb546b]
[zk: localhost:5555(CONNECTED) 5] ls /xd/deployments/modules/2ebbbc9b-63ac-4da4-aa32-e39d69eb546b
[]
[zk: localhost:5555(CONNECTED) 6] ls /xd/containers
[2ebbbc9b-63ac-4da4-aa32-e39d69eb546b]
[zk: localhost:5555(CONNECTED) 7] 
----
The above reflects the initial state of XD. Nothing is deployed yet and there are no stream definitions. Note that _xd/deployments/modules_ has a child which is the id corresponding to the embedded container. If you are running in a clustered environment and connected to one of the ZooKeeper servers in the same ensemble that XD is connected to, you should see multiple nodes under _/xd/containers_ and there may be some existing deployments.

Start the XD Shell in a new terminal session and create a stream:

[source,bash]
----
$ shell/bin/xd-shell
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.0.0.BUILD-SNAPSHOT | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type "help".
xd:>stream create ticktock --definition "time | log"
Created new stream 'ticktock'
xd:>
---- 
Back to the ZK CLI session:

----
[zk: localhost:5555(CONNECTED) 7] ls /xd/streams
[ticktock]
[zk: localhost:5555(CONNECTED) 8] get /xd/streams/ticktock
{"definition":"time | log"}
cZxid = 0x31
ctime = Wed Apr 09 15:22:03 EDT 2014
mZxid = 0x31
mtime = Wed Apr 09 15:22:03 EDT 2014
pZxid = 0x31
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 27
numChildren = 0
[zk: localhost:5555(CONNECTED) 9]
----
using the _get_ command on the new stream node, we can see the stream definition represented as JSON, along with some standard ZooKeeper node information. 

NOTE: _ephemeralOwner = 0x0_, indicating this is not an ephemeral node. At this point, nothing else should have changed from the initial state. 

Now, Using the XD shell, let's deploy the stream,

----
xd>stream deploy ticktock
Deployed stream 'ticktock'
----
and verify with ZooKeeper:

----
[zk: localhost:5555(CONNECTED) 9] ls /xd/deployments/streams
[ticktock]
[zk: localhost:2181(CONNECTED) 10] ls /xd/streams/ticktock
[sink, source]
[zk: localhost:2181(CONNECTED) 11] ls /xd/streams/ticktock/source
[time-0]
[zk: localhost:2181(CONNECTED) 12] ls /xd/streams/ticktock/sink
[log-1]
[zk: localhost:2181(CONNECTED) 13] ls /xd/streams/ticktock/source/time-0
[<container-id>]
[zk: localhost:2181(CONNECTED) 14] ls /xd/streams/ticktock/sink/log-1
[<container-id>]
[zk: localhost:5555(CONNECTED) 15] ls /xd/deployments/modules/<container-id>
[ticktock.sink.log-1, ticktock.source.time-0]
----

Since XD is running as single node, both modules (time and log) are deployed to the same container instance, corresponding to the _<container-id>_. The module node name is _<stream_name>.<module-type>.<module-name>-<module-index>_, where _<module-index>_ represents the position of the module in the stream. 

The information stored in ZooKeeper is provided to XD shell queries. For example:

----
xd:>runtime modules
  Module                  Container Id                          Options
  ----------------------  ------------------------------------  ------------------------------------------
  ticktock.sink.log-1     186d3b36-b005-45ff-b46f-cb2c5cf61ea4
  ticktock.source.time-0  186d3b36-b005-45ff-b46f-cb2c5cf61ea4  {format=yyyy-MM-dd HH:mm:ss, fixedDelay=1}
----

=== Module Deployment

A Stream is composed of Modules. In general, each module is deployed to one or more Container instance(s). In this way the Stream processing is distributed among multiple containers. The Admin decides to which container(s) each Module is deployed and writes the module information to _/xd/deployments/modules/<container-id>_. The Container has a Deploymentlistener to monitor this node for new modules to deploy. If the deployment is successful, the Container writes it's id as an ephemeral node to _xd/streams/<stream_name>/<module-type>/<module-name>-<module-index>/<container-id>_.

image::images/module-deployment.png[XD Admin Internals, width=500]

By default, deploying a stream in a distributed configuration uses simple round robin logic. For example if there are 3 containers and 3 modules in a stream definition,  s1= m1 | m2 | m3,  then XD will attempt distribute the work load evenly among each container. This is a very simplistic strategy and does not take into account things like:

* server load - how many modules are already deployed to a container? How close is it to exhausting available memory, cpu, etc.?
* server affinity - some containers may have external software installed with which specific modules should be co-located. For example, an hdfs sink could be deployed only to servers running Hadoop. Or perhaps a file sink should be deployed to servers configured with more disk space.
* scalability - Suppose the stream s1, above, can achieve higher throughput with multiple instances of m2 running, so we want to deploy m2 to every container.
* fault tolerance - the ability to target physical servers on redundant networks, routers, racks, etc.

==== Deployment Manifest

More complex strategies are critical to tuning and operating XD. Additionally, we must consider various features and constraints when deploying to a PaaS, Yarn or some other cluster manager. Furthermore, such deployment concerns should be addressed independently from the stream definition which is really an expression of the processing logic. To accommodate deployment concerns, XD provides a Deployment Manifest which is submitted with the deployment request, in the form of in-line properties, or a reference to a persisted document containing deployment properties.

When you execute a _stream deploy_ shell command, you can optionally pass a --properties parameter which is a comma delimited list of key=value pairs. Examples for the key include *module.[modulename].count* and *module.[modulename].criteria* (for a full list of properties, see below). The value for a count is a positive integer, and the value for criteria is a valid SpEL expression. The Admin server will match the available containers to the deployment manifest. The stream is considered to be successfully deployed if at least one of each module instance is deployed to a container. For example, 

----
xd:>stream create test1 --definition "http | transform --expression=payload.toUpperCase() | log"
Created new stream 'test1' 
----

Next, deploy it requesting three transformer instances:

----
xd:>stream deploy --name test1 --properties "module.transform.count=3"
Deployed stream 'test1'
----

If there are only two container instances available, only two instances of transform will be deployed. The stream deployment is successful since it is functional. However the unfulfilled deployment request remains active and a third instance will be deployed if a new container comes on line that matches the criteria.

==== Deployment Properties

===== General Properties

module.[modulename].count:: See above.
module.[modulename].criteria:: See above.

===== Bus Properties

====== Common Bus Properties

NOTE: The following properties are only allowed when using a _RabbitMessageBus_ or a _RedisMessageBus_; the _LocalMessageBus_ does not support properties.

module.[modulename].consumer.backOffInitialInterval:: The number of milliseconds to wait for the first delivery retry *(default 1000)*
module.[modulename].consumer.backOffMaxInterval:: The maximum number of milliseconds to wait between retries *(default 10000)*
module.[modulename].consumer.backOffMultiplier:: The previous retry interval is multiplied by this to determine the current interval (but see _backOffMaxInterval_) *(default 2.0)*
module.[modulename].consumer.concurrency:: The number of concurrent consumers for the module *(default 1)*.
module.[modulename].consumer.maxAttempts:: The maximum number of attempts to make a delivery when a failure occurs *(default 3)*

====== RabbitMQ Bus Properties

NOTE: The following properties are only allowed when using a _RabbitMessageBus_.

See the Spring AMQP reference documentation for information about the RabbitMQ-specific attributes.

module.[modulename].consumer.ackMode:: Controls message acknowledgements *(default AUTO)*
module.[modulename].consumer.maxConcurrency:: The maximum number of concurrent consumers for the module *(default 1)*.
module.[modulename].consumer.prefetch:: The number of messages prefetched from the RabbitMQ broker *(default 1)*
module.[modulename].consumer.prefix:: A prefix applied to all queues/exchanges that are declared by the bus - allows policies to be applied *(default 'xdbus.')*
module.[modulename].consumer.requestHeaderPatterns:: Controls which message headers are passed between modules **(default 'STANDARD_REQUEST_HEADERS,*')**
module.[modulename].consumer.replyHeaderPatterns:: Controls which message headers are passed between modules (only used in partitioned jobs) **(default 'STANDARD_REPLY_HEADERS,*')**
module.[modulename].consumer.requeue:: Whether messages will be requeued (and retried) on failure *(default true)*
module.[modulename].consumer.transacted:: Whether consumers use transacted channels *(default false)*
module.[modulename].consumer.txSize:: The number of delivered messages between acknowledgements (when _ackMode=AUTO_) *(default 1)*
module.[modulename].producer.deliveryMode:: THe delivery mode of messages sent to RabbitMQ (_PERSISTENT_ or _NON_PERSISTENT_) *(default PERSISTENT)*
module.[modulename].producer.requestHeaderPatterns:: Controls which message headers are passed between modules **(default 'STANDARD_REQUEST_HEADERS,*')**
module.[modulename].producer.replyHeaderPatterns:: Controls which message headers are passed between modules (only used in partitioned jobs) **(default 'STANDARD_REPLY_HEADERS,*')**


===== Partition Properties

NOTE: The following properties are only allowed when using a _RabbitMessageBus_ or a _RedisMessageBus_.

module.[modulename].producer.partitionKeyExtractorClass:: The class name of a _PartitionKeyExtractorStrategy_ *(default null)*
module.[modulename].producer.partitionKeyExpression:: A _SpEL_ expression, evaluated against the message, to determine the partition key; only applies if _partitionKeyExtractorClass_ is null. If both are null, the module is not partitioned *(default null)*
module.[modulename].producer.partitionSelectorClass:: The class name of a _PartitionSelectorStrategy_ *(default null)*
module.[modulename].producer.partitionSelectorExpression:: A _SpEL_ expression, evaluated against the partition key, to determine the partition index to which the message will be routed. The final partition index will be the return value (an integer) modulo _[nextModule].count_ If both the class and expression are null, the bus's default _PartitionSelectorStrategy_ will be applied to the key *(default null)* 

===== Partitioning

To summarize, a module is partitioned if its _count_ is > 1 and the previous module has a _partitionKeyExtractorClass_ or _partitionKeyExpression_ (class takes precedence). When a partition key is extracted, the partitioned module instance is determined by invoking the _partitionSelectorClass_, if present, or the _partitionSelectorExpression % count_. If neither is present the result is _key.hashCode() % count_.

See below for examples of deploying partitioned modules.

==== Container Attributes

The SpEL context (root object) for the Deployment Manifest is ContainerAtrtributes, basically a map derivative that contains some standard attributes:

 * *id* - the generated container ID
 * *pid* - the process ID of the container instance
 * *host* - the host name of the machine running the container instance
 * *ip* -- the IP address of the machine running the container instance

ContainerAttributes also includes any user-defined attribute values configured for the container. These attributes are configured by editing _xd/config/servers.yml_ the file included in the XD distribution contains some commented out sections as examples. In this case, the container attributes configuration looks something like:

[source, yaml]
----
xd:
  container:
      groups: group2
      color: red
----

NOTE: Groups may also be assigned to a container via the optional command line argument _--groups_ or by setting the environment variable _XD_CONTAINER_GROUPS_. As the property name suggests, a container may belong to more than one group, represented as comma-delimited string. XD considers the concept of groups a useful convention for targeting groups of servers for deployment in a variety of scenarios, so it enjoys special treatment. However, there is nothing technically different from groups and other user defined attribute.


=== Stream Deployment Examples 

To Illustrate how to use the Deployment Manifest, We will use the following runtime configuration, as displayed in the XD shell:

----
xd:>runtime containers
  Container Id                          Host              IP Address     PID   Groups  Custom Attributes
  ------------------------------------  ----------------  -------------  ----  ------  -----------------
  bc624816-f8a8-4f35-83f6-a125ed147b7c  ip-10-110-18-10   10.110.18.10   1708  group2  {color=red}
  018b7c8d-6fa9-4759-8471-76899766f892  ip-10-139-36-168  10.139.36.168  1852  group2  {color=blue}
  afc3741c-217a-415a-9d86-a1f62de03613  ip-10-139-17-116  10.139.17.116  1861  group1  {color=green} 
----

Each of the three containers is running on a different host and has configured Groups and Custom Attributes as shown.

First, create a stream:

----
xd:>stream create test1 --definition "http | transform --expression=payload.toUpperCase() | log"
Created new stream 'test1' 
----

Next, deploy it using a manifest:

----
xd:>stream deploy --name test1 --properties "module.transform.count=3,module.log.criteria=groups.contains('group1')"
Deployed stream 'test1'
----

Verify the deployment:

----
xd:>runtime modules
  Module                       Container Id                          Properties
  ---------------------------  ------------------------------------  ----------------------------------------------
  test1.source.http-0          bc624816-f8a8-4f35-83f6-a125ed147b7c  {port=9000}
  test1.processor.transform-1  bc624816-f8a8-4f35-83f6-a125ed147b7c  {valid=true, expression=payload.toUpperCase()}
  test1.processor.transform-1  018b7c8d-6fa9-4759-8471-76899766f892  {valid=true, expression=payload.toUpperCase()}
  test1.processor.transform-1  afc3741c-217a-415a-9d86-a1f62de03613  {valid=true, expression=payload.toUpperCase()}
  test1.sink.log-2             afc3741c-217a-415a-9d86-a1f62de03613
----

We can see that three instances of the processor have been deployed, one to each container instance. Also the log module has been deployed to the container id corresponding to _group1_. Now we can undeploy and deploy the stream using a different manifest:

----
xd:>stream undeploy test1
Un-deployed stream 'test1'
xd:>runtime modules
  Module  Container Id  Properties
  ------  ------------  ----------

xd:>stream deploy --name test1 --properties "module.log.count=3,module.log.criteria=!groups.contains('group1')"
Deployed stream 'test1'

xd:>runtime modules
  Module                       Container Id                          Properties
  ---------------------------  ------------------------------------  ----------------------------------------------
  test1.sink.log-2             bc624816-f8a8-4f35-83f6-a125ed147b7c
  test1.processor.transform-1  018b7c8d-6fa9-4759-8471-76899766f892  {valid=true, expression=payload.toUpperCase()}
  test1.sink.log-2             018b7c8d-6fa9-4759-8471-76899766f892
  test1.source.http-0          afc3741c-217a-415a-9d86-a1f62de03613  {port=9000}
----

Note that there are only two instances of _log_ deployed. We asked for three however the criteria specified only containers not in _group1_ are eligible. Since only two containers matched the criteria, we have a _log_ module deployed on each one. If we start a new container not in _group1_, the third instance will be deployed. The stream is currently shown as deployed since it is functional even though the manifest is not completely satisfied.


=== Partitioned Stream Deployment Examples

==== Using SpEL Expressions

First, create a stream:

----
xd:>stream create partitioned --definition "jms | transform --expression=#expensiveTransformation(payload) | log"

Created new stream 'partitioned'
----

(hypothetical SpEL function 'expensiveTransformation')

Next, deploy it using a manifest:

----
xd:>stream deploy --name partitioned --properties "module.jms.producer.partitionKeyExpression=payload.customerId,module.transform.count=3"

Deployed stream 'partitioned'
----

In this example three instances of the transformer will be created (with partition index of 0, 1, and 2). When the jms module sends a message it will take the _customerId_ property on the message payload, invoke its _hashCode()_ method and apply the modulo function with the divisor being the _transform.count_ property to determine which instance of the transform will process the message (*payload.getCustomerId().hashCode() % 3*). Messages with the same _customerId_ will always be processed by the same instance.

