=== Introduction
This document describes what's happening "under the hood" of the Spring XD Distributed Runtime (DIRT) and in particular, how the runtime architecture achieves high availability and failover in a clustered production environment. See link:Running-Distributed-Mode[Running in Distributed Mode] for more information on installing and running Spring XD in distributed mode.

This discussion focuses on Spring XD's core runtime components and the role of http://zookeeper.apache.org[ZooKeeper] in managing the state of the Spring XD cluster and enabling automatic recovery from failures.

=== Configuring Spring XD for High Availabilty(HA)

A production Spring XD environment is typically distributed among multiple hosts in a clustered environment. Spring XD scales horizontally when you add container instances. In the simplest case, all containers are replicas, that is each instance is running on an identically configured host and modules are deployed to any available container in a round-robin fashion. However, this simplifying assumption does not address real production scenarios in which more control is requred in order to optimize resource utilization. To this end, Spring XD supports a flexible algorithm which allows you to match module deployments to specific container configurations. The container matching algorithm will be covered in more detail later, but for now, let's assume the simple case. Running multiple containers not only enables horizontal scalability, but enables  failure recovery. If a container becomes unavailable due to an unrecoverable connection loss, any modules currently deployed to that container will be deployed automatically to the other available instances.  

Spring XD requires that a single active Admin server handle interactions with the containers, such as stream deployment requests, as these types of operations must be processed serially in the order received. Without a backup, the Admin server becomes single point of failure. Therefore, two (or more for the risk averse) Admin servers are recommended for a production environment. Note that every Admin server can handle all requests via link:REST-API[REST] endpoints but only one instance, the "Leader", will actually perform requests that update the runtime state. If the Leader goes down, another available Admin server will assume the leader role. http://curator.apache.org/curator-recipes/leader-election.html[Leader Election] is an example of a common feature for distributed systems provided by the http://curator.apache.org[Curator Framework] which sits on top of ZooKeeper. 

An HA Spring XD installation also requires that external servers - ZooKeeper, messaging middleware, and data stores needed for link:Running-Distributed-Mode[running Spring XD in distributed mode] must be configured for HA as well. Please consult the product documentation for specific recommendations regarding each of these external components.

==== HA Messsage Bus

The +RabbitMessageBus+ allows for HA configuration using normal https://www.rabbitmq.com/ha.html[RabbitMQ HA Configuration]. 

First, use the +addresses+ property in +servers.yml+ to include the host/port for each server in the cluster. See link:Application-Configuration#rabbitConfig[Application Configuration].

By default, queues and exchanges declared by the bus are prefixed with +xdbus.+ (this prefix can be changed as described in link:Application-Configuration#rabbitBusProps[Application Configuration]).

To configure the entire bus for HA, create a policy:

+rabbitmqctl set_policy ha-xdbus "^xdbus\." '{"ha-mode":"all"}'+


=== ZooKeeper Overview

In the previous section, we claimed that if a container goes down, Spring XD will redeploy any modules deployed on that instance to another available container. We also claimed that if the Admin Leader goes down, another Admin server will assume that role. http://zookeeper.apache.org[ZooKeeper] is what makes this all possible. ZooKeeper is a widely used Apache project designed primarily for distributed system management and coordination. This section will cover some basic concepts necessary to understand its role in Spring XD. See https://cwiki.apache.org/confluence/display/ZOOKEEPER/Index[The ZooKeeper Wiki] for a more complete overview.

ZooKeeper is based on a simple hierarchical data structure, formally a tree, and conceptually and semantically similar to a file directory structure. As such, data is stored in _nodes_. A node is referenced via a _path_, for example, _/xd/streams/mystream_. Each node can store additional data, serialized as a byte array. In Spring XD, all data is a java.util.Map serialized as JSON. The following figure shows the Spring XD schema:

image::images/zk_xd_schema.png[ZooKeeper XD Schema]

A ZooKeeper node is either _ephemeral_ or _persistent_.  An ephemeral node exists only as long as the session that created it remains active. A persistent node is, well, persistent. Ephemeral nodes are appropriate for registering Container instances. When an Spring XD container starts up it creates an ephemeral node, _/xd/containers/<container-id>_,  using an internally generated container id. When the container's session is closed due to a connection loss, for example, the container process terminates, its node is removed. The ephemeral container node also holds metadata such as its hostname and IP address, runtime metrics, and user defined container attributes. Persistent nodes maintain state needed for normal operation and recovery. This includes data such as stream definitions, job definitions, deployment manifests, module deployments, and deployment state for streams and jobs.

Obviously ZooKeeper is a critical piece of the Spring XD runtime and must itself be HA. ZooKeeper itself supports a distributed architecture, called an _ensemble_. The details are beyond the scope of this document but for the sake of this discussion it is worth mentioning that there should be at least three ZooKeeper server instances running (an odd number is always recommended) on dedicated hosts. The Container and Admin nodes are clients to the ZooKeeper ensemble and must connect to ZooKeeper at startup. Spring XD components are configured with a _zk.client.connect_ property which may designate a single <host>:<port> or a comma separated list. The ZooKeeper client will attempt to connect to each server in order until it succeeds. If it is unable to connect, it will keep trying. If a connection is lost, the ZooKeeper client will attempt to reconnect to one of the servers. The ZooKeeper cluster guarantees consistent replication of data across the ensemble. Specifically, ZooKeeper guarantees:

* Sequential Consistency - Updates from a client will be applied in the order that they were sent.
* Atomicity - Updates either succeed or fail. No partial results.
* Single System Image - A client will see the same view of the service regardless of the server that it connects to.
* Reliability - Once an update has been applied, it will persist from that time forward until a client overwrites the update.
* Timeliness - The clients view of the system is guaranteed to be up-to-date within a certain time bound.

ZooKeeper maintains data primarily in memory backed by a disk cache. Updates are logged to disk for recoverability, and writes are serialized to disk before they are applied to the in-memory database.

In addition to performing basic CRUD operations on nodes, A ZooKeeper client can register a callback on a node to respond to any events or state changes to that node or any of its children. Such node operations and callbacks are the mechanism that control the Spring XD runtime. 

image::images/xd-cluster.png[Spring XD and ZK Ensemble, width=500]

=== The Admin Server Internals

Assuming more than one Admin instance is running, Each instance requests leadership at start up. If there is already a designated leader, the instance will watch the _xd/admin_ node to be notified if the Leader goes away. The instance designated as the "Leader", using the Leader Selector recipe provided by http://curator.apache.org[Curator], a ZooKeeper client library that implements some common patterns. Curator also provides some Listener callback interfaces that the client can register on a node. The AdminServer creates the top level nodes, depicted in the figure above:

* */xd/admins* - children are ephemeral nodes for each available Admin instance and used for Leader Selector 
* */xd/containers* - children are ephemeral nodes containing runtime attributes including hostname,process id, ip address, and user defined attributes for each container instance.
* */xd/streams* - children are persistent nodes containing the definition (DSL) for each stream.
* */xd/jobs* - children are persistent nodes containing the definition (DSL) for each job.
* */xd/taps* - children are persistent nodes describing each deployed tap.
* */xd/deployments/streams* - children are nodes containing stream deployment status (leaf nodes are ephemeral).
* */xd/deployments/jobs* - children are nodes containing job deployment status (leaf nodes are ephemeral).
* */xd/deployments/modules/requested* - stores module deployment requests including deployment criteria.
* */xd/deployments/modules/allocated* - stores information describing currently deployed modules.

The admin leader creates a DeploymentSupervisor which registers listeners on _/xd/deployments/modules/requested_ to handle module deployment requests related to stream and job deployments, and _xd/containers/_ to be notified when containers are added and removed from the cluster. Note that any Admin instance can handle user requests. For example, if you enter the following commands via XD shell,

----
xd>stream create ticktock --definition "time | log"
----  
This command will invoke a REST service on its connected Admin instance to create a new node /xd/streams/ticktock

----
xd>stream deploy ticktock
---- 

Assuming the deployment is successful, This will result in the creation of several nodes used to manage deployed resources, for example, _/xd/deployments/streams/ticktock_. The details are discussed in the <<example-1, example below>>. 

If the Admin instance connected to the shell is not the Leader, it will perform no further action. The Leader's DeploymentSupervisor will attempt to deploy each module in the stream definition, in accordance with the deployment manifest, to an available container, and update the runtime state.

image::images/xd-admin-internals.png[XD Admin Internals]

[[example-1]]
==== Example

Let's walk through the simple example above. If you don't have a Spring XD cluster set up, this example can be easily executed running Spring XD in a single node configuration. The single node application includes an embedded ZooKeeper server by default and allocates a random unused port. The embedded ZooKeeper connect string is reported in the console log for the single node application:

----
...
13:04:27,016  INFO main util.XdConfigLoggingInitializer - Transport: local
13:04:27,016  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop22
13:04:27,019  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath: 2.2.0
*13:04:27,019  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:31316*
...
----

For our purposes, we will use the ZooKeeper CLI tool to inspect the contents of ZooKeeper nodes reflecting the current state of Spring XD. First, we need to know the port to connect the CLI tool to the embedded server. For convenience, we will assign the ZooKeeper port (5555 in this example) when starting the single node application. From the XD install directory:

----
$export JAVA_OPTS="-Dzk.embedded.server.port=5555"
$xd/bin/xd-singlenode
----

In another terminal session, start the ZooKeeper CLI included with ZooKeeper to connect to the embedded server and inspect the contents of the nodes (NOTE: tab completion works) :

----
$zkCli.sh -server localhost:5555
----
After some console output, you should see a prompt:

----
WatchedEvent state:SyncConnected type:None path:null
[zk: localhost:5555(CONNECTED) 0]
----
navigate using the _ls_ command. This will reflect the schema shown in the figure above, the unique container ID will be different for you.

----
[[zk: localhost:5555(CONNECTED) 0] ls /xd
[deployments, containers, admins, taps, streams, jobs]
[zk: localhost:5555(CONNECTED) 1] ls /xd/streams
[]
[zk: localhost:5555(CONNECTED) 2] ls /xd/deployments
[jobs, streams, modules]
[zk: localhost:5555(CONNECTED) 3] ls /xd/deployments/streams
[]
[zk: localhost:5555(CONNECTED) 4] ls /xd/deployments/modules
[requested, allocated]
[zk: localhost:5555(CONNECTED) 5] ls /xd/deployments/modules/allocated
[2ebbbc9b-63ac-4da4-aa32-e39d69eb546b]
[zk: localhost:5555(CONNECTED) 6] ls /xd/deployments/modules/2ebbbc9b-63ac-4da4-aa32-e39d69eb546b
[]
[zk: localhost:5555(CONNECTED) 7] ls /xd/containers
[2ebbbc9b-63ac-4da4-aa32-e39d69eb546b]
[zk: localhost:5555(CONNECTED) 8] 
----
The above reflects the initial state of Spring XD with a running admin and container instance. Nothing is deployed yet and there are no existing stream or job definitions. Note that _xd/deployments/modules/allocated_ has a persistent child corresponding to the id of the container at _xd/containers_. If you are running in a distributed configuration and connected to one of the ZooKeeper servers in the same ensemble that Spring XD is connected to, you might see multiple nodes under _/xd/containers_, and _xd/admins_. Because the external ensemble persists the state of the Spring XD cluster, you will also see any deployments that existed when the Spring XD cluster was shut down.

Start the XD Shell in a new terminal session and create a stream:

[source,bash]
----
$ shell/bin/xd-shell
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.0.0.BUILD-SNAPSHOT | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type "help".
xd:>stream create ticktock --definition "time | log"
Created new stream 'ticktock'
xd:>
---- 
Back to the ZK CLI session:

----
[zk: localhost:5555(CONNECTED) 8] ls /xd/streams
[ticktock]
[zk: localhost:5555(CONNECTED) 9] get /xd/streams/ticktock
{"definition":"time | log"}
cZxid = 0x31
ctime = Mon Jul 14 10:32:33 EDT 2014
mZxid = 0x31
mtime = Mon Jul 14 10:32:33 EDT 2014
pZxid = 0x31
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 27
numChildren = 0
[zk: localhost:5555(CONNECTED) 10]
----
using the _get_ command on the new stream node, we can see the stream definition represented as JSON, along with some standard ZooKeeper metadata. 

NOTE: _ephemeralOwner = 0x0_, indicating this is not an ephemeral node. At this point, nothing else should have changed from the initial state. 

Now, Using the Spring XD shell, let's deploy the stream,

----
xd>stream deploy ticktock
Deployed stream 'ticktock'
----
and verify with ZooKeeper:

----
[zk: localhost:5555(CONNECTED) 10] ls /xd/deployments/streams
[ticktock]
[zk: localhost:2181(CONNECTED) 11] ls /xd/streams/deployments/ticktock
[modules, status]
[[zk: localhost:2181(CONNECTED) 12] get /xd/deployments/streams/ticktock/status
{"state":"deployed"}
....
zk: localhost:2181(CONNECTED) 13] ls /xd/deployments/streams/ticktock/modules
[source.time.1.2ebbbc9b-63ac-4da4-aa32-e39d69eb546b, sink.log.1.2ebbbc9b-63ac-4da4-aa32-e39d69eb546b]
----

Note the deployment state shown for the stream's status node is _deployed_, meaning the deployment request was satisfied. Deployment states are discussed in more detail <<deployment-states,below>>.

Spring XD decomposes stream deployment requests to individual module deployment requests. Hence, we see that each module in the stream is associated with a container instance. The container instance in this case is the same since there is only one instance in the single node configuration. In a distributed configuration with more than one instance, the stream source and sink will each be deployed to a separate container. The node name itself is of the form _<module_type>.<module_name>.<module_sequence_number>.<container_id>_, where the sequence number identifies a deployed instance of a module if multiple instances of that module are requested.

----
[zk: localhost:2181(CONNECTED) 14] ls /xd/deployments/modules/allocated/2ebbbc9b-63ac-4da4-aa32-e39d69eb546b/ticktock.source.time.1
[metadata, status]
----

The _metadata_ and _status_ nodes are ephemeral nodes which store details about the deployed module. This information is provided to XD shell queries. For example:

----
xd:>runtime modules
  Module                  Container Id                          Options                                          Deployment Properties
  ----------------------  ------------------------------------  -----------------------------------------------  ---------------------
  ticktock.sink.log.1     2ebbbc9b-63ac-4da4-aa32-e39d69eb546b  {name=ticktock, expression=payload, level=INFO}  {count=1, sequence=1}
  ticktock.source.time.1  2ebbbc9b-63ac-4da4-aa32-e39d69eb546b  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}       {count=1, sequence=1}
----

=== Module Deployment

A stream is composed of modules. Generally, each module is deployed to one or more Container instance(s). In this way the stream processing is distributed among multiple containers. The _StreamDeploymentListener_ invokes its _ContainerMatcher_ to select a container instance for each module and records the module's deployment properties under _/xd/deployments/modules/requested/_. If a match is found, the StreamDeploymentListener creates a node for the module under _/xd/deployments/modules/allocated/<container_id>_. The Container includes a _DeploymentListener_ that monitors the container node for new modules to deploy. If the deployment is successful, the Container writes the ephemeral nodes _status_ and _metadata_ under the new module node.

image::images/module-deployment.png[Module Deployment]

By default, deploying a stream in a distributed configuration uses simple round robin logic. For example if there are 3 containers and 3 modules in a stream definition,  s1= m1 | m2 | m3,  then XD will attempt distribute the work load evenly among each container. This is a very simplistic strategy and does not take into account things like:

* server load - how many modules are already deployed to a container? How close is it to exhausting available memory, cpu, etc.?
* server affinity - some containers may have external software installed with which specific modules should be colocated. For example, an hdfs sink could be deployed only to servers running Hadoop. Or perhaps a file sink should be deployed to servers configured with more disk space.
* scalability - Suppose the stream s1, above, can achieve higher throughput with multiple instances of m2 running, so we want to deploy m2 to every container.
* fault tolerance - the ability to target physical servers on redundant networks, routers, racks, etc.

==== Deployment Manifest

More complex strategies are needed to tune and operate XD. Additionally, we must consider various features and constraints when deploying to a PaaS, Yarn or some other cluster manager. Such deployment concerns are independent of the stream definition itself which only expresses the processing logic. To addres deployment concerns, Spring XD provides a Deployment Manifest which is submitted with the deployment request, in the form of in-line deployment properties, or a reference to a persisted document containing deployment properties.

When you execute the _stream deploy_ shell command, you can optionally pass a --properties parameter which is a comma delimited list of key=value pairs. Examples for the key include *module.[modulename].count* and *module.[modulename].criteria* (for a full list of properties, see below). The value for a count is a positive integer, and the value for criteria is a valid SpEL expression. The Admin server will match the available containers to the deployment manifest. 

[[deployment-states]]
==== Deployment States

The ability to use criteria to match container instances and specify a number of instances for each module leads to one of several possible deployment states for the stream as a whole. First, consider a newly created stream in an initial _undeployed_ state. 

 image::images/deploy_states.png[Deploy States]

The resulting state of the stream will be one of:

* *Deployed* - All modules deployed successfully.
* *Incomplete* - One of the requested module instances could not be deployed, but at least one instance of each module was deployed. The stream is considered operational but the deployment request was not completely satisfied.
* *Failed* - No instances were deployed for at least one of the modules.

NOTE: The state diagram implies these states are final. This is not accurate since these states are volatile in reality. When container goes down or a new one arrives, transitions among these "final" states will occur. Such transitions have been omitted from the diagram to keep things simple. Also, there is a symmetric state model for undeploying a stream from any one of these states which is not covered here. 

===== Example
----
xd:>stream create test1 --definition "http | transform --expression=payload.toUpperCase() | log"
Created new stream 'test1' 
----

Next, deploy it requesting three transformer instances:

----
xd:>stream deploy --name test1 --properties "module.transform.count=3"
Deployed stream 'test1'

xd:>stream list
  Stream Name  Stream Definition                                          Status
  -----------  ---------------------------------------------------------  ----------
  test1        http | transform --expression=payload.toUpperCase() | log  incomplete
----

If there are only two container instances available, only two instances of _transform_ will be deployed. The stream deployment state is _incomplete_ and the stream is functional. However the unfulfilled deployment request remains active and the third instance will be deployed if a new container comes on line that matches the criteria.

==== Deployment Properties

===== General Properties

NOTE: You can apply criteria to all modules by using the wildcard _*_ for [modulename]

module.[modulename].count:: The number of module instances (see above).
module.[modulename].criteria:: A SpEL expression using the <<container-attributes, container attributes>> as an evaluation context.

----
xd:>stream deploy --name test1 --properties "module.transform.count=3,module.log.criteria=groups.contains('group1')"
----

===== Bus Properties

====== Common Bus Properties

NOTE: The following properties are only allowed when using a _RabbitMessageBus_ or a _RedisMessageBus_; the _LocalMessageBus_ does not support properties.

module.[modulename].consumer.backOffInitialInterval:: The number of milliseconds to wait for the first delivery retry *(default 1000)*
module.[modulename].consumer.backOffMaxInterval:: The maximum number of milliseconds to wait between retries *(default 10000)*
module.[modulename].consumer.backOffMultiplier:: The previous retry interval is multiplied by this to determine the current interval (but see _backOffMaxInterval_) *(default 2.0)*
module.[modulename].consumer.concurrency:: The number of concurrent consumers for the module *(default 1)*.
module.[modulename].consumer.maxAttempts:: The maximum number of attempts to make a delivery when a failure occurs *(default 3)*

====== Direct Binding

Sometimes it is desirable to colocate all modules in a stream, allowing the modules to communicate direcly rather than using the configured remote transport. This eliminates network latency, possibly at the expense of reliabile messaging features provided by Rabbit MQ for example. Spring XD supports direct binding at the module level. Direct binding happens during module deployment only in cases where we know that every "pair" of producer and consumer (modules bound on either side of a pipe) are guaranteed to be colocated with each other. Currently Spring XD has no conditional logic to force modules to be colocated - in fact it does the opposite. The only way to guarantee that every producer-consumer pair is colocated is to specify that the pair be deployed to every available container instance, in other words, the module counts must be 0. The figure below illustrates this concept. In the first hypothetical case, we deploy one instance (the default) of producer m1, and two instances of the consumer m2. In this case, enabling direct binding would isolate one of the consumer instances. Spring XD will not use direct binding in this case. The second case guarantees colocation of the pairs and will result in direct binding. 

image::images/direct-binding.png[Direct Binding]

In addition, direct binding requires that the producer is not configured for <<partition-properties,partitioning>> since partitioning is implemented by the Message Bus.

Using _module.\*.count=0_ is the most straightforward way to enable direct binding. Direct binding may be disabled for the stream using _module.*.producer.directBinding=false_. Additional <<direct-binding, direct binding deployment examples>> are shown below.


====== RabbitMQ Bus Properties

NOTE: The following properties are only allowed when using a _RabbitMessageBus_.

See the Spring AMQP reference documentation for information about the RabbitMQ-specific attributes.

module.[modulename].consumer.ackMode:: Controls message acknowledgements *(default AUTO)*
module.[modulename].consumer.maxConcurrency:: The maximum number of concurrent consumers for the module *(default 1)*.
module.[modulename].consumer.prefetch:: The number of messages prefetched from the RabbitMQ broker *(default 1)*
module.[modulename].consumer.prefix:: A prefix applied to all queues/exchanges that are declared by the bus - allows policies to be applied *(default 'xdbus.')*
module.[modulename].consumer.requestHeaderPatterns:: Controls which message headers are passed between modules **(default 'STANDARD_REQUEST_HEADERS,*')**
module.[modulename].consumer.replyHeaderPatterns:: Controls which message headers are passed between modules (only used in partitioned jobs) **(default 'STANDARD_REPLY_HEADERS,*')**
module.[modulename].consumer.requeue:: Whether messages will be requeued (and retried) on failure *(default true)*
module.[modulename].consumer.transacted:: Whether consumers use transacted channels *(default false)*
module.[modulename].consumer.txSize:: The number of delivered messages between acknowledgements (when _ackMode=AUTO_) *(default 1)*
module.[modulename].producer.deliveryMode:: THe delivery mode of messages sent to RabbitMQ (_PERSISTENT_ or _NON_PERSISTENT_) *(default PERSISTENT)*
module.[modulename].producer.requestHeaderPatterns:: Controls which message headers are passed between modules **(default 'STANDARD_REQUEST_HEADERS,*')**
module.[modulename].producer.replyHeaderPatterns:: Controls which message headers are passed between modules (only used in partitioned jobs) **(default 'STANDARD_REPLY_HEADERS,*')**

[[partition-properties]]
===== Partition Properties

NOTE: The following properties are only allowed when using a _RabbitMessageBus_ or a _RedisMessageBus_.

module.[modulename].producer.partitionKeyExtractorClass:: The class name of a _PartitionKeyExtractorStrategy_ *(default null)*
module.[modulename].producer.partitionKeyExpression:: A _SpEL_ expression, evaluated against the message, to determine the partition key; only applies if _partitionKeyExtractorClass_ is null. If both are null, the module is not partitioned *(default null)*
module.[modulename].producer.partitionSelectorClass:: The class name of a _PartitionSelectorStrategy_ *(default null)*
module.[modulename].producer.partitionSelectorExpression:: A _SpEL_ expression, evaluated against the partition key, to determine the partition index to which the message will be routed. The final partition index will be the return value (an integer) modulo _[nextModule].count_ If both the class and expression are null, the bus's default _PartitionSelectorStrategy_ will be applied to the key *(default null)* 

===== Partitioning

To summarize, a module is partitioned if its _count_ is > 1 and the previous module has a _partitionKeyExtractorClass_ or _partitionKeyExpression_ (class takes precedence). When a partition key is extracted, the partitioned module instance is determined by invoking the _partitionSelectorClass_, if present, or the _partitionSelectorExpression % count_. If neither is present the result is _key.hashCode() % count_.

See below for examples of deploying <<partitioned-streams,partitioned streams>>.

[[container-attributes]]
==== Container Attributes

The SpEL context (root object) for module.[modulename].criteria is ContainerAtrtributes, basically a map derivative that contains some standard attributes:

 * *id* - the generated container ID
 * *pid* - the process ID of the container instance
 * *host* - the host name of the machine running the container instance
 * *ip* -- the IP address of the machine running the container instance

ContainerAttributes also includes any user-defined attribute values configured for the container. These attributes are configured by editing _xd/config/servers.yml_ the file included in the XD distribution contains some commented out sections as examples. In this case, the container attributes configuration looks something like:

[source, yaml]
----
xd:
  container:
      groups: group2
      color: red
----

NOTE: Groups may also be assigned to a container via the optional command line argument _--groups_ or by setting the environment variable _XD_CONTAINER_GROUPS_. As the property name suggests, a container may belong to more than one group, represented as comma-delimited string. The concept of server groups is considered an especially useful convention for targeting groups of servers for deployment to support many common scenarios, so it enjoys special status. Internally, _groups_ is simply a user defined attribute.

=== Stream Deployment Examples 

To Illustrate how to use the Deployment Manifest, We will use a runtime configuration with 3 container instances, as displayed in the XD shell:

----
xd:>runtime containers
  Container Id                          Host              IP Address     PID   Groups  Custom Attributes
  ------------------------------------  ----------------  -------------  ----  ------  -----------------
  bc624816-f8a8-4f35-83f6-a125ed147b7c  ip-10-110-18-10   10.110.18.10   1708  group2  {color=red}
  018b7c8d-6fa9-4759-8471-76899766f892  ip-10-139-36-168  10.139.36.168  1852  group2  {color=blue}
  afc3741c-217a-415a-9d86-a1f62de03613  ip-10-139-17-116  10.139.17.116  1861  group1  {color=green} 
----

Each of the three containers is running on a different host and has configured Groups and Custom Attributes as shown.

First, create a stream:

----
xd:>stream create test1 --definition "http | transform --expression=payload.toUpperCase() | log"
Created new stream 'test1' 
----

Next, deploy it using a manifest:

----
xd:>stream deploy --name test1 --properties "module.transform.count=3,module.log.criteria=groups.contains('group1')"
Deployed stream 'test1'
----

Verify the deployment:

----
xd:>runtime modules
  Module                       Container Id                          Options                                         Deployment Properties
  ---------------------------  ------------------------------------  ----------------------------------------------  ---------------------------------------------------------
  test1.processor.transform.1  bc624816-f8a8-4f35-83f6-a125ed147b7c  {valid=true, expression=payload.toUpperCase()}  {count=3, sequence=1}
  test1.processor.transform.2  018b7c8d-6fa9-4759-8471-76899766f892  {valid=true, expression=payload.toUpperCase()}  {count=3, sequence=2}
  test1.processor.transform.3  afc3741c-217a-415a-9d86-a1f62de03613  {valid=true, expression=payload.toUpperCase()}  {count=3, sequence=3}
  test1.sink.log.1             afc3741c-217a-415a-9d86-a1f62de03613  {name=test1, expression=payload, level=INFO}    {count=1, sequence=1, criteria=groups.contains('group1')}
  test1.source.http.1          bc624816-f8a8-4f35-83f6-a125ed147b7c  {port=9000}                                     {count=1, sequence=1}
----

We can see that three instances of the _transform_ processor have been deployed, one to each container instance. Also the log module has been deployed to the container assigned to _group1_. Now we can undeploy and deploy the stream using a different manifest:

----
xd:>stream undeploy test1
Un-deployed stream 'test1'
xd:>runtime modules
  Module  Container Id  Properties
  ------  ------------  ----------

xd:>stream deploy --name test1 --properties "module.log.count=3,module.log.criteria=!groups.contains('group1')"
Deployed stream 'test1'

xd:>stream list
  Stream Name  Stream Definition                                          Status
  -----------  ---------------------------------------------------------  ----------
  test1        http | transform --expression=payload.toUpperCase() | log  incomplete

xd:>runtime modules
  Module                       Container Id                          Options                                         Deployment Properties
  ---------------------------  ------------------------------------  ----------------------------------------------  ----------------------------------------------------------
  test1.processor.transform.1  018b7c8d-6fa9-4759-8471-76899766f892  {valid=true, expression=payload.toUpperCase()}  {count=1, sequence=1}
  test1.sink.log.1             bc624816-f8a8-4f35-83f6-a125ed147b7c  {name=test1, expression=payload, level=INFO}    {count=3, sequence=1, criteria=!groups.contains('group1')}
  test1.sink.log.2             018b7c8d-6fa9-4759-8471-76899766f892  {name=test1, expression=payload, level=INFO}    {count=3, sequence=2, criteria=!groups.contains('group1')}
  test1.source.http.1          afc3741c-217a-415a-9d86-a1f62de03613  {port=9000}                                     {count=1, sequence=1}


----

Now there are only two instances of the _log_ module deployed. We asked for three however the deployment criteria specifies only containers not in _group1_ are eligible. The _log_ module is deployed only to the two containers matching the criteria. The deploymnent status of stream _test1_ is shown as _incomplete_. The stream is functional even though the deployment manifest is not completely satisfied. If we fire up a new container not in _group1_, the DeploymentSupervisor will handle any outstanding deployment requests by comparing _xd/deployments/modules/requested_ to _xd/deployments/modules/allocated_, and will deploy the third _log_ instance and update the stream state to _deployed_. 

[[partitioned-streams]]
=== Partitioned Stream Deployment Examples

==== Using SpEL Expressions

First, create a stream:

----
xd:>stream create --name partitioned --definition "jms | transform --expression=#expensiveTransformation(payload) | log"

Created new stream 'partitioned'
----

The hypothetical SpEL function 'expensiveTransformation' represents a resource intensive processor which we want to load balance by running on multiple containers. In this case, we also want to partition the stream so that payloads containing the same _customerId_ are always routed to the same processor instance. Perhaps the processor aggregates data by customerId and this step needs to run using colocated resources.

Next, deploy it using a manifest:

----
xd:>stream deploy --name partitioned --properties "module.jms.producer.partitionKeyExpression=payload.customerId,module.transform.count=3"

Deployed stream 'partitioned'
----

In this example three instances of the transformer will be created (with partition index of 0, 1, and 2). When the jms module sends a message it will take the _customerId_ property on the message payload, invoke its _hashCode()_ method and apply the modulo function with the divisor being the _transform.count_ property to determine which instance of the transform will process the message (*payload.getCustomerId().hashCode() % 3*). Messages with the same _customerId_ will always be processed by the same instance.

[[direct-binding]]
=== Direct Binding Deployment Examples

In the simplest case, we enforce direct binding by setting the instance count to 0 for all modules in the stream. A count of 0 means deploy the module to all available containers:

----
xd:>runtime containers
  Container Id                          Host            IP Address    PID    Groups  Custom Attributes
  ------------------------------------  --------------  ------------  -----  ------  -----------------
  8e814924-15de-4ca1-82d3-ddfe851668ab  ultrafox.local  192.168.1.18  81532
  a2b89274-2d40-46e4-afc5-4988bea28a16  ultrafox.local  192.168.1.9   4605   group1
----

We start with two container instances. One belongs to the group _group1_.

----
xd:>stream create direct --definition "time | log"
Created new stream 'direct'
xd:>stream deploy direct --properties module.*.count=0
Deployed stream 'direct'
xd:>runtime modules
  Module                Container Id                          Options                                        Deployment Properties
  --------------------  ------------------------------------  ---------------------------------------------  ---------------------------------------------------------
  direct.sink.log.0     a2b89274-2d40-46e4-afc5-4988bea28a16  {name=direct, expression=payload, level=INFO}  {count=0, sequence=0}
  direct.sink.log.0     8e814924-15de-4ca1-82d3-ddfe851668ab  {name=direct, expression=payload, level=INFO}  {count=0, sequence=0}
  direct.source.time.0  a2b89274-2d40-46e4-afc5-4988bea28a16  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}     {producer.directBindingAllowed=true, count=0, sequence=0}
  direct.source.time.0  8e814924-15de-4ca1-82d3-ddfe851668ab  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}     {producer.directBindingAllowed=true, count=0, sequence=0}
----

Note that we have two containers and two instances of each module deployed to each. Spring XD automatically sets the bus properties needed to allow direct binding, _producer.directBindingAllowed=true_ on the _time_ module. 

Suppose we only want one instance of this stream and we want it to use direct binding. Here we can add deployment criteria to restrict the available containers to _group1_.

----
xd:>stream undeploy direct
Un-deployed stream 'direct'
xd:>stream deploy direct --properties "module.*.count=0, module.*.criteria=groups.contains('group1')"
Deployed stream 'direct'
xd:>runtime modules
  Module                Container Id                          Options                                        Deployment Properties
  --------------------  ------------------------------------  ---------------------------------------------  ---------------------------------------------------------------------------------------------
  direct.sink.log.0     a2b89274-2d40-46e4-afc5-4988bea28a16  {name=direct, expression=payload, level=INFO}  {count=0, sequence=0, criteria=groups.contains('group1')}
  direct.source.time.0  a2b89274-2d40-46e4-afc5-4988bea28a16  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}     {producer.directBindingAllowed=true, count=0, sequence=0, criteria=groups.contains('group1')}
----

Direct binding eliminates latency between modules but sacrifices some of the resiliency provided by the messaging middleware. In the scenario above, if we lose one of the containers, we lose messages. To disable direct binding when module counts are set to 0, set _module.*.producer.directBindingAllowed=false_. 

----
xd:>stream undeploy direct
Un-deployed stream 'direct'
xd:>stream deploy direct --properties "module.*.count=0, module.*.producer.directBindingAllowed=false"
Deployed stream 'direct'
xd:>runtime modules
  Module                Container Id                          Options                                        Deployment Properties
  --------------------  ------------------------------------  ---------------------------------------------  ----------------------------------------------------------
  direct.sink.log.0     a2b89274-2d40-46e4-afc5-4988bea28a16  {name=direct, expression=payload, level=INFO}  {producer.directBindingAllowed=false, count=0, sequence=0}
  direct.sink.log.0     8e814924-15de-4ca1-82d3-ddfe851668ab  {name=direct, expression=payload, level=INFO}  {producer.directBindingAllowed=false, count=0, sequence=0}
  direct.source.time.0  a2b89274-2d40-46e4-afc5-4988bea28a16  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}     {producer.directBindingAllowed=false, count=0, sequence=0}
  direct.source.time.0  8e814924-15de-4ca1-82d3-ddfe851668ab  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}     {producer.directBindingAllowed=false, count=0, sequence=0}
----

Finally, we can still have the best of both worlds by enabling guarenteed delivery at one point in the stream, usually the source. If the tail of the stream is colocated and the source uses the message bus, the message bus may be configured so that if a container instance goes down, any unacknowledged messages will be retried until the container comes back or its modules are redeployed.

#TODO some examples....

An alternate scenario with similar characteristics would be if the stream uses a _rabbit_ or _jms_ source. In this case, guaranteed delivery would be configured in the external messaging system instead of the Spring XD transport. 


=== Error Handling (Message Delivery Failures)

Note: the following applies to normally deployed streams. When direct binding between modules is being used, exceptions thrown by the consumer are thrown back to the producer.

==== RabbitMQ Message Bus

When a consuming module (processor, sink) fails to handle a message, the bus will retry delivery based on the module (or default bus) retry configuration. The default configuration will make 3 attempts to deliver the message. The retry configuration can be modified at the bus level (in servers.yml), or for an individual stream/module using the deployment manifest.

When retries are exhausted, by default, messages are discarded. However, using RabbitMQ, you can configure such messages to be routed to a dead-letter exchange/dead letter queue. See the https://www.rabbitmq.com/dlx.html[RabbitMQ Documentation] for more information.

Consider a stream: +stream create foo --definition "source | processor | sink"+

The first _pipe_ (by default) will be backed by a queue named +xdbus.foo.0+, the second by +xdbus.foo.1+. Messages are routed to these queues using the default exchange (with routing keys equal to the queue names).

To enable dead lettering just for this stream, first configure a policy:

+rabbitmqctl set_policy foo.DLX "^xdbus\.foo\..*" '{"dead-letter-exchange":"foo.dlx"}' --apply-to queues+

To configure dead-lettering for all streams:

+rabbitmqctl set_policy DLX "^xdbus\..*" '{"dead-letter-exchange":"dlx"}' --apply-to queues+

The next step is to declare the dead letter exchange, and bind dead letter queues with the appropriate routing keys.

For example, for the second "pipe" in the stream above we might bind a queue +foo.sink.dlq+ to exchange +foo.dlx+ with a routing key +xdbus.foo.1+ (remember, the original routing key was the queue name).

Now, when the sink fails to handle a message, after the configured retries are exhausted, the failed message will be routed to +foo.sink.dlq+.

There is no automated mechanism provided to move dead lettered messages back to the bus queue.


==== Redis Message Bus

When Redis is the transport, the failed messages (after retries are exhausted) are +LPUSH+ed to a +LIST ERRORS:<stream>.n+ (e.g. +ERRORS:foo.1+ in the above example in the _RabbitMQ Message Bus_ section).

This is unconditional; the data in the +ERRORS LIST+ is in "bus" format; again, some external mechanism would be needed to move the data from the ERRORS LIST back to the bus's foo.1 LIST.
